{
  "hash": "98cfd0f85dae39046df97dfa46eba10c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Green Maritime Corridors - Canal de Panamá\n---\n\n\n\nquarto preview\n\n## Librerías y dependencias\n\n\n::: {#librerias .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_selection import SelectKBest, f_regression\n```\n:::\n\n\n## Lectura y limpieza inicial\n\n::: {#lectura_limpieza_inicial .cell execution_count=2}\n``` {.python .cell-code}\n# Lectura y limpieza inicial\nruta_data = r\"C:\\Users\\limco\\Documents\\DATA_C\\sample_training.parquet\"\ndf_raw = pd.read_parquet(ruta_data, engine='fastparquet')\n\n# Limpiar nombres de columnas\ndf_raw.columns = (\n    df_raw.columns.str.strip()\n                  .str.replace(' ', '_')\n                  .str.replace(r'[^\\w]', '', regex=True)\n)\n\n# Convertir columna de fecha\ndf_raw['first_dt_pos_utc'] = pd.to_datetime(df_raw['first_dt_pos_utc'])\n```\n:::\n\n\n## Partición externa\n\n::: {#particion_externa .cell execution_count=3}\n``` {.python .cell-code}\n# Partición externa\nfecha_corte = pd.to_datetime('2019-06-01')\ntrain_val = df_raw[df_raw['first_dt_pos_utc'] < fecha_corte].copy()\ntest_ext  = df_raw[df_raw['first_dt_pos_utc'] >= fecha_corte].copy()\n```\n:::\n\n\n# Exploración inicial (EDA)\n\n## Resumen de train\\_val\n\n::: {#resumen_train_val .cell message='false' execution_count=4}\n``` {.python .cell-code}\nfrom tabulate import tabulate\n\nprint(f\"Cantidad de filas en train_val: {len(train_val)}\")\nprint(f\"Cantidad de columnas en train_val: {len(train_val.columns)}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCantidad de filas en train_val: 27532\nCantidad de columnas en train_val: 46\n\n```\n:::\n:::\n\n\n## Exploración inicial de datos (sin h3\\_sequence)\n\n::: {#exploracion_i .cell message='false' execution_count=5}\n``` {.python .cell-code}\nfrom tabulate import tabulate\n\n# Columnas a mostrar (excluyendo 'h3_sequence')\ncols_to_show = [c for c in train_val.columns if c != 'h3_sequence']\n\n# Mostrar primeras filas\nprint(tabulate(train_val[cols_to_show].head(5), headers='keys', tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+-----------+---------------------------+---------------------------+---------+---------------------+---------------------+---------------------------+------------------+--------------+-----------------+-----------------+------------------------+------------------+---------------------+----------------+--------------+--------------+---------+------------------+--------------+--------------------+---------------+-------+--------------+-------------------+---------+------------+----------------+------------------+-----------------+---------------------+---------------------+----------------------------+-----------------+------------------+------------------+-----------------------------+------------------+-----------------------+------------------------+----------+--------+----------+------------+-----------+\n|    |      mmsi | start_leg                 | end_leg                   |   imo_x | ref_in              | ref_out             | port_before               | country_before   | port_after   | country_after   | op_phase        | StandardVesselType_x   |   GrossTonnage_x | first_dt_pos_utc    |     sum_me_ene |   sum_ae_ene |   sum_ab_ene |   imo_y |   GrossTonnage_y |   Deadweight |   LengthOverallLOA |   DateOfBuild |   TEU |   Powerkwmax | MainEngineModel   |   Speed |   Speedmax |   Speedservice |   BreadthExtreme |   SummerDraught |   FuelType1Capacity |   FuelType2Capacity |   LightDisplacementTonnage |   MainEngineRPM | MainEngineType   |   Powerkwservice | PropulsionType              | ShiptypeLevel5   |   TotalBunkerCapacity | StandardVesselType_y   |   imobin | fuel   | meType   |   ais_beam |   ais_loa |\n|----+-----------+---------------------------+---------------------------+---------+---------------------+---------------------+---------------------------+------------------+--------------+-----------------+-----------------+------------------------+------------------+---------------------+----------------+--------------+--------------+---------+------------------+--------------+--------------------+---------------+-------+--------------+-------------------+---------+------------+----------------+------------------+-----------------+---------------------+---------------------+----------------------------+-----------------+------------------+------------------+-----------------------------+------------------+-----------------------+------------------------+----------+--------+----------+------------+-----------|\n|  0 | 205681000 | 2019-02-10 22:02:21+00:00 | 2019-02-23 20:29:37+00:00 | 9659139 | 2019-02-18 08:16:03 | 2019-02-18 14:51:33 | BAHIA_QUINTERO_(VENTANAS) | CL               | PORT_ARANSAS | US              | Normal cruising | Liquified gas tanker   |            25143 | 2019-02-10 22:05:12 |    1.11291e+06 |  38922.9     |   16217.9    | 9659139 |            25143 |        28590 |              174   |        201501 |     0 |         8360 | 6S50ME-B9         |    16.8 |        0   |           16.8 |           30.03  |          10.3   |                 156 |                2083 |                      11354 |              99 | Oil              |                0 | Oil Engine(s), Direct Drive | LPG Tanker       |                  2239 | Liquified gas tanker   |        1 | HFO    | SSD      |         30 |       174 |\n|  1 | 205681000 | 2019-02-10 22:02:21+00:00 | 2019-02-23 20:29:37+00:00 | 9659139 | 2019-02-18 08:16:03 | 2019-02-18 14:51:33 | BAHIA_QUINTERO_(VENTANAS) | CL               | PORT_ARANSAS | US              | Anchorage       | Liquified gas tanker   |            25143 | 2019-02-11 17:58:45 |    0           |   3169.8     |    2641.5    | 9659139 |            25143 |        28590 |              174   |        201501 |     0 |         8360 | 6S50ME-B9         |    16.8 |        0   |           16.8 |           30.03  |          10.3   |                 156 |                2083 |                      11354 |              99 | Oil              |                0 | Oil Engine(s), Direct Drive | LPG Tanker       |                  2239 | Liquified gas tanker   |        1 | HFO    | SSD      |         30 |       174 |\n|  2 | 205681000 | 2019-02-10 22:02:21+00:00 | 2019-02-23 20:29:37+00:00 | 9659139 | 2019-02-18 08:16:03 | 2019-02-18 14:51:33 | BAHIA_QUINTERO_(VENTANAS) | CL               | PORT_ARANSAS | US              | Slow transit    | Liquified gas tanker   |            25143 | 2019-02-13 02:40:16 | 1107.73        |    390.467   |     162.694  | 9659139 |            25143 |        28590 |              174   |        201501 |     0 |         8360 | 6S50ME-B9         |    16.8 |        0   |           16.8 |           30.03  |          10.3   |                 156 |                2083 |                      11354 |              99 | Oil              |                0 | Oil Engine(s), Direct Drive | LPG Tanker       |                  2239 | Liquified gas tanker   |        1 | HFO    | SSD      |         30 |       174 |\n|  3 | 205681000 | 2019-02-10 22:02:21+00:00 | 2019-02-23 20:29:37+00:00 | 9659139 | 2019-02-18 08:16:03 | 2019-02-18 14:51:33 | BAHIA_QUINTERO_(VENTANAS) | CL               | PORT_ARANSAS | US              | Manoeuvring     | Liquified gas tanker   |            25143 | 2019-02-18 07:50:28 |   17.5365      |     55.6     |      30.8889 | 9659139 |            25143 |        28590 |              174   |        201501 |     0 |         8360 | 6S50ME-B9         |    16.8 |        0   |           16.8 |           30.03  |          10.3   |                 156 |                2083 |                      11354 |              99 | Oil              |                0 | Oil Engine(s), Direct Drive | LPG Tanker       |                  2239 | Liquified gas tanker   |        1 | HFO    | SSD      |         30 |       174 |\n|  4 | 205760000 | 2019-01-23 07:35:52+00:00 | 2019-03-11 03:18:59+00:00 | 9402017 | 2019-02-24 06:50:06 | 2019-02-24 13:36:16 | MIZUSHIMA_KO              | JP               | ST._JAMES    | US              | Normal cruising | Bulk carrier           |            30619 | 2019-02-08 20:55:45 |   18.9087      |      1.37222 |       0      | 9336983 |            79235 |       149876 |              274.2 |        200802 |     0 |        16440 | 6RTA72            |    12.3 |       15.4 |           12.3 |           48.041 |          16.022 |                 340 |                3830 |                      22770 |              94 | Oil              |                0 | Oil Engine(s), Direct Drive | Crude Oil Tanker |                  4170 | Oil tanker             |        7 | HFO    | SSD      |         48 |       274 |\n+----+-----------+---------------------------+---------------------------+---------+---------------------+---------------------+---------------------------+------------------+--------------+-----------------+-----------------+------------------------+------------------+---------------------+----------------+--------------+--------------+---------+------------------+--------------+--------------------+---------------+-------+--------------+-------------------+---------+------------+----------------+------------------+-----------------+---------------------+---------------------+----------------------------+-----------------+------------------+------------------+-----------------------------+------------------+-----------------------+------------------------+----------+--------+----------+------------+-----------+\n```\n:::\n:::\n\n\n## Revisión de tipos de datos\n\n::: {#tipos_columnas .cell message='false' execution_count=6}\n``` {.python .cell-code}\nfrom tabulate import tabulate\n\ntipos = train_val.dtypes.reset_index()\ntipos.columns = ['Columna', 'Tipo de dato']\n\nprint(tabulate(tipos, headers='keys', tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+--------------------------+---------------------+\n|    | Columna                  | Tipo de dato        |\n|----+--------------------------+---------------------|\n|  0 | mmsi                     | int32               |\n|  1 | start_leg                | datetime64[ns, UTC] |\n|  2 | end_leg                  | datetime64[ns, UTC] |\n|  3 | h3_sequence              | object              |\n|  4 | imo_x                    | int32               |\n|  5 | ref_in                   | datetime64[us]      |\n|  6 | ref_out                  | datetime64[us]      |\n|  7 | port_before              | object              |\n|  8 | country_before           | object              |\n|  9 | port_after               | object              |\n| 10 | country_after            | object              |\n| 11 | op_phase                 | object              |\n| 12 | StandardVesselType_x     | object              |\n| 13 | GrossTonnage_x           | int32               |\n| 14 | first_dt_pos_utc         | datetime64[us]      |\n| 15 | sum_me_ene               | float64             |\n| 16 | sum_ae_ene               | float64             |\n| 17 | sum_ab_ene               | float64             |\n| 18 | imo_y                    | int32               |\n| 19 | GrossTonnage_y           | int32               |\n| 20 | Deadweight               | int32               |\n| 21 | LengthOverallLOA         | float64             |\n| 22 | DateOfBuild              | int32               |\n| 23 | TEU                      | int32               |\n| 24 | Powerkwmax               | float64             |\n| 25 | MainEngineModel          | object              |\n| 26 | Speed                    | float64             |\n| 27 | Speedmax                 | float64             |\n| 28 | Speedservice             | float64             |\n| 29 | BreadthExtreme           | float64             |\n| 30 | SummerDraught            | float64             |\n| 31 | FuelType1Capacity        | float64             |\n| 32 | FuelType2Capacity        | float64             |\n| 33 | LightDisplacementTonnage | int32               |\n| 34 | MainEngineRPM            | float64             |\n| 35 | MainEngineType           | object              |\n| 36 | Powerkwservice           | int32               |\n| 37 | PropulsionType           | object              |\n| 38 | ShiptypeLevel5           | object              |\n| 39 | TotalBunkerCapacity      | float64             |\n| 40 | StandardVesselType_y     | object              |\n| 41 | imobin                   | int64               |\n| 42 | fuel                     | object              |\n| 43 | meType                   | object              |\n| 44 | ais_beam                 | float64             |\n| 45 | ais_loa                  | float64             |\n+----+--------------------------+---------------------+\n```\n:::\n:::\n\n\n## Distribución de registros por buque\n\n::: {#eda_secuencias .cell message='false' execution_count=7}\n``` {.python .cell-code}\nfrom tabulate import tabulate\n\nregistros_por_buque = train_val.groupby(\"mmsi\").size()\ntabla_freq = registros_por_buque.value_counts().sort_index()\ntabla_freq_df = tabla_freq.reset_index()\ntabla_freq_df.columns = [\"n_registros_por_buque\", \"n_buques\"]\ntabla_freq_df = tabla_freq_df.sort_values(by=\"n_buques\", ascending=False).reset_index(drop=True)\n```\n:::\n\n\nLa tabla muestra que la mayoría de los buques tienen pocos registros, la mayor parte entre 1 y 5 pasos. Solo unos pocos buques tienen muchos registros (más de 50), lo que indica que unos pocos son muy activos mientras la mayoría realiza trayectorias cortas o tiene menos datos registrados\n\n::: {#a_secuencias .cell message='false' execution_count=8}\n``` {.python .cell-code}\nprint(\"\\nTabla de registros por buque ordenada por n_buques:\")\nprint(tabulate(tabla_freq_df, headers='keys', tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTabla de registros por buque ordenada por n_buques:\n+----+-------------------------+------------+\n|    |   n_registros_por_buque |   n_buques |\n|----+-------------------------+------------|\n|  0 |                       4 |        588 |\n|  1 |                       1 |        381 |\n|  2 |                       5 |        380 |\n|  3 |                       9 |        172 |\n|  4 |                       2 |        166 |\n|  5 |                       8 |        158 |\n|  6 |                      10 |        123 |\n|  7 |                       3 |         92 |\n|  8 |                       6 |         65 |\n|  9 |                      13 |         55 |\n| 10 |                      15 |         39 |\n| 11 |                      14 |         38 |\n| 12 |                      18 |         36 |\n| 13 |                       7 |         31 |\n| 14 |                      17 |         28 |\n| 15 |                      19 |         24 |\n| 16 |                      12 |         24 |\n| 17 |                      11 |         23 |\n| 18 |                      16 |         19 |\n| 19 |                      20 |         19 |\n| 20 |                      25 |         17 |\n| 21 |                      35 |         17 |\n| 22 |                      21 |         14 |\n| 23 |                      24 |         13 |\n| 24 |                      39 |         12 |\n| 25 |                      22 |         11 |\n| 26 |                      30 |         11 |\n| 27 |                      29 |         10 |\n| 28 |                      32 |          8 |\n| 29 |                      40 |          8 |\n| 30 |                      41 |          7 |\n| 31 |                      28 |          7 |\n| 32 |                      23 |          7 |\n| 33 |                      45 |          7 |\n| 34 |                      27 |          6 |\n| 35 |                      49 |          6 |\n| 36 |                      33 |          6 |\n| 37 |                      50 |          6 |\n| 38 |                      55 |          6 |\n| 39 |                      26 |          5 |\n| 40 |                      38 |          5 |\n| 41 |                      54 |          5 |\n| 42 |                      44 |          5 |\n| 43 |                      36 |          5 |\n| 44 |                      31 |          4 |\n| 45 |                      51 |          4 |\n| 46 |                      48 |          4 |\n| 47 |                      34 |          4 |\n| 48 |                      42 |          4 |\n| 49 |                      56 |          4 |\n| 50 |                      71 |          4 |\n| 51 |                      74 |          4 |\n| 52 |                      57 |          3 |\n| 53 |                      62 |          3 |\n| 54 |                      65 |          3 |\n| 55 |                      43 |          2 |\n| 56 |                      59 |          2 |\n| 57 |                      69 |          2 |\n| 58 |                      60 |          2 |\n| 59 |                      53 |          2 |\n| 60 |                      37 |          2 |\n| 61 |                      75 |          2 |\n| 62 |                      81 |          2 |\n| 63 |                      63 |          2 |\n| 64 |                      87 |          2 |\n| 65 |                      61 |          1 |\n| 66 |                      58 |          1 |\n| 67 |                      47 |          1 |\n| 68 |                      52 |          1 |\n| 69 |                      72 |          1 |\n| 70 |                      70 |          1 |\n| 71 |                      64 |          1 |\n| 72 |                      77 |          1 |\n| 73 |                      79 |          1 |\n| 74 |                      80 |          1 |\n| 75 |                      82 |          1 |\n| 76 |                      66 |          1 |\n| 77 |                      85 |          1 |\n| 78 |                      86 |          1 |\n| 79 |                      91 |          1 |\n| 80 |                      94 |          1 |\n| 81 |                      98 |          1 |\n| 82 |                     100 |          1 |\n| 83 |                     104 |          1 |\n| 84 |                     110 |          1 |\n| 85 |                     116 |          1 |\n| 86 |                     122 |          1 |\n| 87 |                     132 |          1 |\n| 88 |                     137 |          1 |\n| 89 |                     139 |          1 |\n| 90 |                     149 |          1 |\n| 91 |                     195 |          1 |\n| 92 |                     240 |          1 |\n+----+-------------------------+------------+\n```\n:::\n:::\n\n\n### Gráfico de distribución de registros por buque\n\n::: {#cell-eda_grafico .cell message='false' execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntabla_freq_df['n_buques_acum'] = tabla_freq_df['n_buques'].cumsum()\n\nplt.figure(figsize=(12,5))\nsns.barplot(\n    data=tabla_freq_df, \n    x='n_registros_por_buque', \n    y='n_buques', \n    color='steelblue'\n)\nplt.xlabel(\"Número de registros por buque\")\nplt.ylabel(\"Número de buques\")\nplt.title(\"Distribución de registros por buque (train_val)\")\nplt.xticks(rotation=90)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/eda_grafico-output-1.png){#eda_grafico}\n:::\n:::\n\n\n## Verificar viajes puerto–puerto\n\n::: {#eda_viajes_puerto .cell message='false' execution_count=10}\n``` {.python .cell-code}\nviajes_incompletos = train_val[\n    train_val['port_before'].isna() | \n    train_val['port_after'].isna()  |\n    (train_val['port_before'].str.lower().isin(['', 'desconocido'])) |\n    (train_val['port_after'].str.lower().isin(['', 'desconocido']))\n]\n\nprint(\"Cantidad de registros con viajes incompletos (puertos faltantes o desconocidos):\", len(viajes_incompletos))\nprint(tabulate(\n    viajes_incompletos[['mmsi','port_before','port_after']].head(10),\n    headers='keys',\n    tablefmt='psql'\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCantidad de registros con viajes incompletos (puertos faltantes o desconocidos): 0\n+--------+---------------+--------------+\n| mmsi   | port_before   | port_after   |\n|--------+---------------+--------------|\n+--------+---------------+--------------+\n```\n:::\n:::\n\n\n**“Todos los registros de viajes de puerto a puerto están completos; cada buque tiene correctamente registrado su puerto de salida y de llegada, sin información faltante ni desconocida.”**\n\n\n\n\n## Revisar consumos de energía\n\n::: {#eda_consumos .cell message='false' execution_count=11}\n``` {.python .cell-code}\ncols_energia = ['sum_me_ene','sum_ae_ene','sum_ab_ene']\nresumen_consumos = []\n\nfor col in cols_energia:\n    n_nulos = train_val[col].isna().sum()\n    n_cero  = (train_val[col] == 0).sum()\n    n_neg   = (train_val[col] < 0).sum()\n    resumen_consumos.append([col, n_nulos, n_cero, n_neg])\n```\n:::\n\n\n::: {#e_consumos .cell message='false' execution_count=12}\n``` {.python .cell-code}\nprint(\"Valores nulos, cero y negativos en consumos:\")\nprint(tabulate(resumen_consumos, headers=[\"Columna\",\"Nulos\",\"Ceros\",\"Negativos\"], tablefmt=\"psql\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValores nulos, cero y negativos en consumos:\n+------------+---------+---------+-------------+\n| Columna    |   Nulos |   Ceros |   Negativos |\n|------------+---------+---------+-------------|\n| sum_me_ene |      71 |   13973 |           0 |\n| sum_ae_ene |       0 |    7513 |           0 |\n| sum_ab_ene |       0 |   15753 |           0 |\n+------------+---------+---------+-------------+\n```\n:::\n:::\n\n\nLos consumos de energía son consistentes: ninguna columna tiene valores negativos, aunque `sum_me_ene` presenta 71 valores nulos y 13,973 registros con consumo cero, `sum_ae_ene` tiene 7,513 registros con consumo cero, y `sum_ab_ene` cuenta con 15,753 registros con consumo cero.\n\n---\n\n::: {#d_consumos .cell message='false' execution_count=13}\n``` {.python .cell-code}\nprint(\"\\nResumen estadístico de consumos energéticos:\")\nprint(tabulate(train_val[cols_energia].describe().reset_index(), headers='keys', tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nResumen estadístico de consumos energéticos:\n+----+---------+------------------+------------------+--------------+\n|    | index   |       sum_me_ene |       sum_ae_ene |   sum_ab_ene |\n|----+---------+------------------+------------------+--------------|\n|  0 | count   |  27461           |  27532           |    27532     |\n|  1 | mean    | 339900           |  33129.4         |     3261.32  |\n|  2 | std     |      1.54909e+06 | 117658           |    14657.8   |\n|  3 | min     |      0           |      0           |        0     |\n|  4 | 25%     |      0           |      0           |        0     |\n|  5 | 50%     |      0           |   1539.12        |        0     |\n|  6 | 75%     |   7838.64        |  16867.7         |      542.319 |\n|  7 | max     |      4.96151e+07 |      4.54976e+06 |   665954     |\n+----+---------+------------------+------------------+--------------+\n```\n:::\n:::\n\n\nLos consumos de energía varían mucho entre los registros. Muchos viajes registran cero consumo, mientras que unos pocos tienen consumos muy altos, lo que eleva el promedio. En general, la mayoría de los viajes usan poca energía, pero existen algunos casos con consumos significativamente mayores.\n\n\n\n## Duración de viajes puerto–puerto\n\n::: {#eda_duracion_viajes .cell message='false' execution_count=14}\n``` {.python .cell-code}\nmmsi_validos = train_val['mmsi'].value_counts()\nmmsi_validos = mmsi_validos[mmsi_validos >= 12].index\ndf_filtrado = train_val[train_val['mmsi'].isin(mmsi_validos)].copy()\ndf_filtrado['duracion_viaje_horas'] = (\n    (df_filtrado['end_leg'] - df_filtrado['start_leg']).dt.total_seconds() / 3600\n)\n```\n:::\n\n\nLa duración de los tramos de viaje entre puertos varía mucho: van desde unas 44 horas hasta más de 2,000 horas, con la mayoría de los tramos entre 200 y 600 horas y un promedio de unas 430 horas\n\n::: {#duracion_horas .cell message='false' execution_count=15}\n``` {.python .cell-code}\nprint(\"Distribución de duración de viajes (horas):\")\nprint(tabulate(df_filtrado['duracion_viaje_horas'].describe().reset_index(),\n               headers=['Métrica','Valor'],\n               tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDistribución de duración de viajes (horas):\n+----+-----------+------------+\n|    | Métrica   |      Valor |\n|----+-----------+------------|\n|  0 | count     | 17389      |\n|  1 | mean      |   429.841  |\n|  2 | std       |   285.03   |\n|  3 | min       |    43.9656 |\n|  4 | 25%       |   202.708  |\n|  5 | 50%       |   364.996  |\n|  6 | 75%       |   604.001  |\n|  7 | max       |  2087.47   |\n+----+-----------+------------+\n```\n:::\n:::\n\n\nLa duración de los viajes varía ampliamente, desde unas 44 horas hasta más de 2,000 horas, con la mayoría de los viajes entre 200 y 600 horas y un promedio de aproximadamente 430 horas.\n\n::: {#eda_frecuencia_duracion_dias .cell message='false' execution_count=16}\n``` {.python .cell-code}\ndf_filtrado['duracion_viaje_dias'] = (\n    (df_filtrado['end_leg'] - df_filtrado['start_leg']).dt.total_seconds() / (3600*24)\n).astype(int)\n\ntabla_frec = df_filtrado['duracion_viaje_dias'].value_counts().reset_index()\ntabla_frec.columns = ['Duración (días)', 'Cantidad de viajes']\ntabla_frec = tabla_frec.sort_values('Cantidad de viajes', ascending=False)\n```\n:::\n\n\n::: {#duracion_dias .cell message='false' execution_count=17}\n``` {.python .cell-code}\nprint(\"Frecuencia de duración de viajes (en días) ordenada de mayor a menor cantidad de viajes:\")\nprint(tabulate(tabla_frec, headers='keys', tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFrecuencia de duración de viajes (en días) ordenada de mayor a menor cantidad de viajes:\n+----+-------------------+----------------------+\n|    |   Duración (días) |   Cantidad de viajes |\n|----+-------------------+----------------------|\n|  0 |                 6 |                 1084 |\n|  1 |                 5 |                 1026 |\n|  2 |                 7 |                  999 |\n|  3 |                11 |                  825 |\n|  4 |                15 |                  810 |\n|  5 |                10 |                  682 |\n|  6 |                12 |                  679 |\n|  7 |                 9 |                  667 |\n|  8 |                16 |                  645 |\n|  9 |                 8 |                  638 |\n| 10 |                 4 |                  547 |\n| 11 |                26 |                  516 |\n| 12 |                14 |                  515 |\n| 13 |                27 |                  492 |\n| 14 |                13 |                  488 |\n| 15 |                23 |                  461 |\n| 16 |                17 |                  411 |\n| 17 |                18 |                  411 |\n| 18 |                24 |                  407 |\n| 19 |                25 |                  398 |\n| 20 |                19 |                  382 |\n| 21 |                29 |                  361 |\n| 22 |                20 |                  356 |\n| 23 |                28 |                  325 |\n| 24 |                 3 |                  315 |\n| 25 |                22 |                  306 |\n| 26 |                30 |                  290 |\n| 27 |                32 |                  279 |\n| 28 |                21 |                  260 |\n| 29 |                33 |                  254 |\n| 30 |                31 |                  192 |\n| 31 |                34 |                  138 |\n| 32 |                35 |                  133 |\n| 33 |                40 |                   94 |\n| 34 |                61 |                   85 |\n| 35 |                41 |                   77 |\n| 36 |                39 |                   70 |\n| 37 |                57 |                   66 |\n| 38 |                36 |                   66 |\n| 39 |                37 |                   64 |\n| 40 |                38 |                   56 |\n| 41 |                 2 |                   48 |\n| 42 |                43 |                   48 |\n| 43 |                45 |                   36 |\n| 44 |                42 |                   36 |\n| 45 |                52 |                   36 |\n| 46 |                49 |                   28 |\n| 47 |                47 |                   22 |\n| 48 |                71 |                   21 |\n| 49 |                59 |                   20 |\n| 50 |                54 |                   18 |\n| 51 |                68 |                   18 |\n| 52 |                51 |                   18 |\n| 53 |                46 |                   16 |\n| 54 |                65 |                   13 |\n| 55 |                66 |                   13 |\n| 56 |                44 |                   13 |\n| 57 |                74 |                   12 |\n| 58 |                62 |                   11 |\n| 59 |                72 |                   10 |\n| 60 |                58 |                   10 |\n| 61 |                48 |                   10 |\n| 62 |                55 |                   10 |\n| 63 |                56 |                    9 |\n| 64 |                63 |                    9 |\n| 65 |                64 |                    9 |\n| 66 |                69 |                    9 |\n| 67 |                53 |                    8 |\n| 68 |                 1 |                    4 |\n| 69 |                70 |                    2 |\n| 70 |                86 |                    1 |\n| 71 |                78 |                    1 |\n+----+-------------------+----------------------+\n```\n:::\n:::\n\n\nLa mayoría de los tramos de viaje entre puertos duran entre 5 y 7 días, siendo 6 días el más frecuente. Sin embargo, también hay viajes mucho más largos, algunos de hasta 86 días, lo que muestra que mientras la mayoría de los tramos son relativamente cortos, existen casos con duraciones significativamente mayores\n\n::: {#eda_duracion_horas .cell message='false' execution_count=18}\n``` {.python .cell-code}\ndf_filtrado = df_filtrado[\n    df_filtrado['port_before'].notna() &\n    df_filtrado['port_after'].notna() &\n    (~df_filtrado['port_before'].str.lower().isin(['', 'desconocido'])) &\n    (~df_filtrado['port_after'].str.lower().isin(['', 'desconocido']))\n].copy()\n\ndf_filtrado['duracion_horas'] = (\n    (df_filtrado['end_leg'] - df_filtrado['start_leg']).dt.total_seconds() / 3600\n)\n\ntabla_frec_horas = df_filtrado.groupby(['port_before','port_after','duracion_horas']).size().reset_index(name='cantidad_viajes')\ntabla_frec_horas = tabla_frec_horas.sort_values('cantidad_viajes', ascending=False)\n```\n:::\n\n\n::: {#on_horas .cell message='false' execution_count=19}\n``` {.python .cell-code}\ntabla_frec_horas_top10 = tabla_frec_horas.head(10)\nprint(\"Frecuencia de duración de viajes (horas) puerto->puerto ordenada de mayor a menor cantidad de viajes:\")\nprint(tabulate(tabla_frec_horas_top10, headers='keys', tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFrecuencia de duración de viajes (horas) puerto->puerto ordenada de mayor a menor cantidad de viajes:\n+------+-----------------+------------------+------------------+-------------------+\n|      | port_before     | port_after       |   duracion_horas |   cantidad_viajes |\n|------+-----------------+------------------+------------------+-------------------|\n|  427 | CHIRIQUI_GRANDE | CHIRIQUI_GRANDE  |          568.726 |                74 |\n| 1007 | MARIEL          | PUERTO_DE_HENCAN |         1476.04  |                65 |\n| 1912 | SANTA_MARTA     | GEORGETOWN       |          814.829 |                49 |\n|  423 | CHIRIQUI_GRANDE | CHIRIQUI_GRANDE  |          324.607 |                45 |\n| 1238 | PAITA           | TAURANGA         |         1376.92  |                40 |\n|  537 | EL_BOSQUE       | PUERTO_BOLIVAR   |          179.317 |                40 |\n|  591 | FREEPORT        | KINGSTON         |          585.148 |                40 |\n|  424 | CHIRIQUI_GRANDE | CHIRIQUI_GRANDE  |          397.629 |                39 |\n| 1544 | PUERTO_CORTES   | MARIEL           |         1005.14  |                35 |\n|  666 | GALVESTON       | PUERTO_BOLIVAR   |          431.959 |                25 |\n+------+-----------------+------------------+------------------+-------------------+\n```\n:::\n:::\n\n\nLos datos muestran los **tramos de viaje más frecuentes entre puertos** y su duración en horas. Por ejemplo, los viajes dentro de **Chiriquí Grande** son los más repetidos, con varias duraciones registradas (alrededor de 325–570 horas). Otros tramos frecuentes incluyen rutas como **Mariel → Puerto de Hencan** o **Santa Marta → Georgetown**, con duraciones más largas, entre 800 y 1,500 horas. Esto indica que algunos trayectos son recorridos muy a menudo, mientras que otros, aunque menos frecuentes, implican viajes más largos.\n\n::: {#eda_duracion_dias .cell message='false' execution_count=20}\n``` {.python .cell-code}\ndf_filtrado['duracion_dias'] = ((df_filtrado['end_leg'] - df_filtrado['start_leg']).dt.total_seconds() / (3600*24)).astype(int)\ntabla_frec_dias = df_filtrado.groupby(['port_before','port_after','duracion_dias']).size().reset_index(name='cantidad_viajes')\ntabla_frec_dias = tabla_frec_dias.sort_values('cantidad_viajes', ascending=False)\n```\n:::\n\n\n::: {#puerto .cell message='false' execution_count=21}\n``` {.python .cell-code}\ntabla_frec_dias_top10 = tabla_frec_dias.head(10)\nprint(\"Top 10 frecuencia de duración de viajes (días) puerto->puerto ordenada de mayor a menor cantidad de viajes:\")\nprint(tabulate(tabla_frec_dias_top10, headers='keys', tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 10 frecuencia de duración de viajes (días) puerto->puerto ordenada de mayor a menor cantidad de viajes:\n+------+------------------------------+------------------------------+-----------------+-------------------+\n|      | port_before                  | port_after                   |   duracion_dias |   cantidad_viajes |\n|------+------------------------------+------------------------------+-----------------+-------------------|\n|  623 | MANZANILLO                   | KINGSTON                     |              10 |               206 |\n| 1054 | PUERTO_SAN_ANTONIO           | FREEPORT                     |              11 |               165 |\n|   39 | ANDRES_(ANDRES_LNG_TERMINAL) | BUENAVENTURA                 |               6 |               150 |\n|  166 | BUENAVENTURA                 | ANDRES_(ANDRES_LNG_TERMINAL) |               5 |               137 |\n|  911 | PUERTO_BOLIVAR               | ROTTERDAM                    |              16 |               136 |\n|  985 | PUERTO_MARITIMO_DE_GUAYAQUIL | PUERTO_MARITIMO_DE_GUAYAQUIL |               5 |               134 |\n| 1259 | TAURANGA                     | PHILADELPHIA                 |              26 |               131 |\n|  695 | MIAMI                        | PUERTO_MARITIMO_DE_GUAYAQUIL |               7 |               124 |\n| 1206 | SINES                        | MANZANILLO                   |              15 |               121 |\n|    8 | ACAJUTLA                     | PAITA                        |               5 |               114 |\n+------+------------------------------+------------------------------+-----------------+-------------------+\n```\n:::\n:::\n\n\nLos datos muestran los **trayectos puerto a puerto más frecuentes y su duración en días**. Por ejemplo, el viaje **Manzanillo → Kingston** es el más común, con 206 registros y una duración de 10 días. Otros trayectos frecuentes incluyen **Puerto San Antonio → Freeport** (11 días) y **Andrés LNG Terminal ↔ Buenaventura** (5–6 días). Esto refleja que algunos recorridos son muy habituales, mientras que otros, aunque menos frecuentes, pueden durar hasta varias semanas.\n\n\n## Análisis de viajes específicos puerto–puerto\n\n::: {#eda_inspeccionar .cell message='false' execution_count=22}\n``` {.python .cell-code}\npuerto_inicio = \"MANZANILLO\"\npuerto_fin    = \"KINGSTON\"\ndias_viaje    = 10\n\nviajes_filtrados = df_filtrado[\n    (df_filtrado['port_before'] == puerto_inicio) &\n    (df_filtrado['port_after'] == puerto_fin) &\n    (((df_filtrado['end_leg'] - df_filtrado['start_leg']).dt.total_seconds() / (3600*24)).astype(int) == dias_viaje)\n]\n\nresumen_mmsi = viajes_filtrados.groupby('mmsi').size().reset_index(name='registros_viaje')\n\nprint(f\"Barcos que hicieron el viaje {puerto_inicio} -> {puerto_fin} en {dias_viaje} días:\")\nprint(tabulate(resumen_mmsi, headers='keys', tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBarcos que hicieron el viaje MANZANILLO -> KINGSTON en 10 días:\n+----+-------------+-------------------+\n|    |        mmsi |   registros_viaje |\n|----+-------------+-------------------|\n|  0 | 2.18643e+08 |                30 |\n|  1 | 2.48426e+08 |                15 |\n|  2 | 2.48473e+08 |                16 |\n|  3 | 2.56937e+08 |                30 |\n|  4 | 2.56938e+08 |                14 |\n|  5 | 2.5694e+08  |                15 |\n|  6 | 2.56968e+08 |                25 |\n|  7 | 5.38006e+08 |                20 |\n|  8 | 5.6695e+08  |                11 |\n|  9 | 6.36016e+08 |                30 |\n+----+-------------+-------------------+\n```\n:::\n:::\n\n\nEl análisis muestra qué barcos realizaron el trayecto **Manzanillo → Kingston en exactamente 10 días**. Por ejemplo, algunos barcos completaron el viaje varias veces, con registros que van de 11 a 30 tramos por barco. Esto indica que hay rutas muy recurrentes y que ciertos buques realizan este viaje de manera repetida dentro del periodo analizado.\n\n\n\n## Cuántos buques tienen secuencias\n\n::: {#n_buques .cell message='false' execution_count=23}\n``` {.python .cell-code}\nregistros_por_buque = train_val.groupby(\"mmsi\").size()\nlongitudes = [1, 5, 10, 12, 15, 20]\nresumen_secuencias = []\ntotal_buques = len(registros_por_buque)\n\nfor l in longitudes:\n    n_buques = (registros_por_buque >= l).sum()\n    porcentaje = n_buques / total_buques * 100\n    resumen_secuencias.append([l, n_buques, porcentaje])\n\nresumen_df = pd.DataFrame(resumen_secuencias, columns=['Min Registros', 'N° Buques', 'Porcentaje (%)'])\nprint(resumen_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min Registros  N° Buques  Porcentaje (%)\n0              1       2754      100.000000\n1              5       1527       55.446623\n2             10        721       26.180102\n3             12        575       20.878722\n4             15        458       16.630356\n5             20        312       11.328976\n```\n:::\n:::\n\n\nEl análisis muestra **cuántos buques tienen distintos niveles de registros en los datos**. Todos los buques (2,754) aparecen al menos una vez, pero conforme pedimos más registros por buque, el número disminuye: alrededor de la mitad tiene 5 o más registros, un cuarto tiene 10 o más, y solo un pequeño grupo (11 %) tiene 20 o más registros. Esto indica que mientras muchos buques aparecen pocas veces, algunos cuentan con series de datos más largas y consistentes.\n\n::: {#registros_por .cell message='false' execution_count=24}\n``` {.python .cell-code}\nlongitudes = [5, 6, 12, 15]\nregistros_por_buque = train_val.groupby(\"mmsi\").size()\n\nfor n in longitudes:\n    n_buques = (registros_por_buque >= n).sum()\n    print(f\"Buques con al menos {n} registros: {n_buques}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuques con al menos 5 registros: 1527\nBuques con al menos 6 registros: 1147\nBuques con al menos 12 registros: 575\nBuques con al menos 15 registros: 458\n```\n:::\n:::\n\n\n::: {#registros_por_buque .cell message='false' execution_count=25}\n``` {.python .cell-code}\nlongitudes = [ 3,4, 5,6,7,8, 12]\n\nfor n in longitudes:\n    buques_validos = registros_por_buque[registros_por_buque >= n].index\n    df_filtrado = train_val[train_val['mmsi'].isin(buques_validos)]\n    n_filas = len(df_filtrado)\n    n_buques = len(buques_validos)\n    print(f\"Secuencia {n}: {n_buques} buques, {n_filas} filas\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSecuencia 3: 2207 buques, 26819 filas\nSecuencia 4: 2115 buques, 26543 filas\nSecuencia 5: 1527 buques, 24191 filas\nSecuencia 6: 1147 buques, 22291 filas\nSecuencia 7: 1082 buques, 21901 filas\nSecuencia 8: 1051 buques, 21684 filas\nSecuencia 12: 575 buques, 17389 filas\n```\n:::\n:::\n\n\n## Creación de la variable objetivo: CO2\\_emission\n\n::: {#crear_target_emisiones .cell message='false' execution_count=26}\n``` {.python .cell-code}\ndef crear_target_emisiones(df, sfoc_me=0.2, sfoc_ae=0.22, sfoc_ab=0.25, factor_emision=3.17):\n    df = df.copy()\n    df['combustible_me'] = df['sum_me_ene'] * sfoc_me\n    df['combustible_ae'] = df['sum_ae_ene'] * sfoc_ae\n    df['combustible_ab'] = df['sum_ab_ene'] * sfoc_ab\n    df['fuel_consumption'] = df['combustible_me'] + df['combustible_ae'] + df['combustible_ab']\n    df['CO2_emission'] = df['fuel_consumption'] * factor_emision\n    return df\n```\n:::\n\n\n::: {#target_emisiones .cell message='false' execution_count=27}\n``` {.python .cell-code}\ntrain_val_filtrado = crear_target_emisiones(train_val)\ntarget = 'CO2_emission'\n```\n:::\n\n\n### **Bloque 1: Ver ceros en CO2\\_emission**\n\n::: {#inspeccionar_ceros .cell message='false' execution_count=28}\n``` {.python .cell-code}\n# Contar ceros en la variable objetivo\nceros_target = (train_val_filtrado[target] == 0).sum()\nporc_ceros = ceros_target / len(train_val_filtrado) * 100\n\nprint(f\"Registros con CO2_emission = 0: {ceros_target} ({porc_ceros:.2f}%)\")\n\n# Inspeccionar algunos registros con cero\nprint(\"\\nPrimeros registros con CO2_emission = 0:\")\nprint(train_val_filtrado[train_val_filtrado[target] == 0].head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRegistros con CO2_emission = 0: 7500 (27.24%)\n\nPrimeros registros con CO2_emission = 0:\n         mmsi                 start_leg                   end_leg  \\\n32  232013522 2019-01-19 05:34:50+00:00 2019-02-15 03:45:41+00:00   \n34  232013522 2019-01-19 05:34:50+00:00 2019-02-15 03:45:41+00:00   \n36  232013522 2019-01-19 05:34:50+00:00 2019-02-15 03:45:41+00:00   \n38  232013522 2019-01-19 05:34:50+00:00 2019-02-15 03:45:41+00:00   \n40  232013522 2019-04-06 02:54:28+00:00 2019-05-03 09:04:38+00:00   \n\n                                          h3_sequence    imo_x  \\\n32  [582173814721347583, 582314552209702911, 58278...  9362401   \n34  [582173814721347583, 582314552209702911, 58278...  9362401   \n36  [582173814721347583, 582314552209702911, 58278...  9362401   \n38  [582173814721347583, 582314552209702911, 58278...  9362401   \n40  [581720815930703871, 582173814721347583, 58231...  9362401   \n\n                ref_in             ref_out port_before country_before  \\\n32 2019-01-25 14:28:08 2019-01-25 21:54:15  CHARLESTON             US   \n34 2019-01-25 14:28:08 2019-01-25 21:54:15  CHARLESTON             US   \n36 2019-01-25 14:28:08 2019-01-25 21:54:15  CHARLESTON             US   \n38 2019-01-25 14:28:08 2019-01-25 21:54:15  CHARLESTON             US   \n40 2019-04-12 15:01:40 2019-04-12 22:53:59  CHARLESTON             US   \n\n   port_after  ... imobin fuel meType  ais_beam ais_loa  combustible_me  \\\n32   TAURANGA  ...      4  HFO    SSD      32.0   254.0             0.0   \n34   TAURANGA  ...      4  HFO    SSD      32.0   254.0             0.0   \n36   TAURANGA  ...      4  HFO    SSD      32.0   254.0             0.0   \n38   TAURANGA  ...      4  HFO    SSD      32.0   254.0             0.0   \n40   TAURANGA  ...      4  HFO    SSD      32.0   254.0             0.0   \n\n    combustible_ae  combustible_ab  fuel_consumption  CO2_emission  \n32             0.0             0.0               0.0           0.0  \n34             0.0             0.0               0.0           0.0  \n36             0.0             0.0               0.0           0.0  \n38             0.0             0.0               0.0           0.0  \n40             0.0             0.0               0.0           0.0  \n\n[5 rows x 51 columns]\n```\n:::\n:::\n\n\n::: {#mmsi_con_ceros .cell message='false' execution_count=29}\n``` {.python .cell-code}\n# Filtrar registros con CO2_emission = 0\ndf_ceros = train_val_filtrado[train_val_filtrado[target] == 0].copy()\n\n# Cuántos MMSI distintos tienen registros con cero\nmmsi_ceros = df_ceros['mmsi'].nunique()\nprint(f\"Número de buques (mmsi) con al menos un CO2_emission = 0: {mmsi_ceros}\")\n\n# Contar registros con cero por buque\nregistros_ceros_por_mmsi = df_ceros.groupby('mmsi').size().reset_index(name='registros_cero')\nprint(\"\\nRegistros con CO2_emission = 0 por buque (top 10):\")\nprint(registros_ceros_por_mmsi.sort_values('registros_cero', ascending=False).head(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNúmero de buques (mmsi) con al menos un CO2_emission = 0: 797\n\nRegistros con CO2_emission = 0 por buque (top 10):\n          mmsi  registros_cero\n317  370120000             209\n638  563587000             125\n620  563058900              88\n746  636017164              75\n325  370837000              71\n21   212917000              66\n483  477711300              63\n10   210159000              61\n619  563056800              60\n669  566278000              58\n```\n:::\n:::\n\n\n::: {#mmsi_con_ceros_todos .cell message='false' execution_count=30}\n``` {.python .cell-code}\nprint(registros_ceros_por_mmsi.sort_values('registros_cero', ascending=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          mmsi  registros_cero\n317  370120000             209\n638  563587000             125\n620  563058900              88\n746  636017164              75\n325  370837000              71\n..         ...             ...\n105  244870440               1\n428  477013200               1\n429  477027400               1\n430  477030800               1\n5    209393000               1\n\n[797 rows x 2 columns]\n```\n:::\n:::\n\n\n::: {#resumen_mmsi_ceros .cell message='false' execution_count=31}\n``` {.python .cell-code}\n# Total de buques únicos\ntotal_mmsi = train_val_filtrado['mmsi'].nunique()\n\n# Buques con al menos un cero\nmmsi_con_ceros = train_val_filtrado.loc[train_val_filtrado['CO2_emission'] == 0, 'mmsi'].nunique()\n\n# Buques sin ceros\nmmsi_sin_ceros = total_mmsi - mmsi_con_ceros\n\n# Armar tabla resumen\nresumen_mmsi = pd.DataFrame({\n    \"grupo\": [\"Con ceros\", \"Sin ceros\"],\n    \"count_mmsi\": [mmsi_con_ceros, mmsi_sin_ceros],\n    \"porcentaje\": [mmsi_con_ceros/total_mmsi*100, mmsi_sin_ceros/total_mmsi*100]\n})\n\nprint(f\"Total buques únicos (mmsi): {total_mmsi}\\n\")\nprint(resumen_mmsi.to_string(index=False, formatters={\"porcentaje\": \"{:.2f}%\".format}))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal buques únicos (mmsi): 2754\n\n    grupo  count_mmsi porcentaje\nCon ceros         797     28.94%\nSin ceros        1957     71.06%\n```\n:::\n:::\n\n\ncasi 3 de cada 10 buques pasan por situaciones de “cero emisiones” (posible parada, fondeo, error o falta de consumo registrado).\n\n### **Eliminar registros con cero**\n\n::: {#eliminar_ceros .cell message='false' execution_count=32}\n``` {.python .cell-code}\n# Eliminar filas donde CO2_emission es cero\ntrain_val_filtrado = train_val_filtrado[train_val_filtrado[target] != 0].copy()\n\nprint(f\"Filas después de eliminar ceros en CO2_emission: {len(train_val_filtrado)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFilas después de eliminar ceros en CO2_emission: 20032\n```\n:::\n:::\n\n\n## Limpieza de NaN en target y verificación de secuencias mínimas\n\n::: {#limpieza_nan_target .cell message='false' execution_count=33}\n``` {.python .cell-code}\n# Filtrar filas donde CO2_emission es NaN\ntrain_val_target = train_val_filtrado[train_val_filtrado[target].notna()].copy()\n\nseq_len = 4  # longitud mínima de secuencia\n\n# Contar buques válidos antes y después de eliminar NaN\nregistros_por_buque_orig = train_val_filtrado.groupby('mmsi').size()\nbuques_validos_orig = (registros_por_buque_orig >= seq_len).sum()\n\nregistros_por_buque_filtrado = train_val_target.groupby('mmsi').size()\nbuques_validos_filtrado = (registros_por_buque_filtrado >= seq_len).sum()\n\nresumen = pd.DataFrame({\n    'Dataset': ['Original', 'Filtrado por target'],\n    'Buques válidos': [buques_validos_orig, buques_validos_filtrado],\n    'Filas totales': [len(train_val_filtrado), len(train_val_target)]\n})\n\nprint(tabulate(resumen, headers='keys', tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---------------------+------------------+-----------------+\n|    | Dataset             |   Buques válidos |   Filas totales |\n|----+---------------------+------------------+-----------------|\n|  0 | Original            |             2086 |           20032 |\n|  1 | Filtrado por target |             2074 |           19961 |\n+----+---------------------+------------------+-----------------+\n```\n:::\n:::\n\n\nAl filtrar los valores faltantes, quedarán 19,961 registros y 2,074 buques válidos. Esto significará que se habrán eliminado 71 registros individuales y 12 buques completos que ya no tendrán suficientes datos para formar secuencias mínimas.\n\n\n\n## Recorte de buques que no cumplen la longitud mínima\n\n::: {#recorte_buques_min .cell message='false' execution_count=34}\n``` {.python .cell-code}\n# Función para filtrar buques por cantidad mínima de registros\ndef filtrar_buques_por_min_registros(df, min_registros=4):\n    registros_por_buque = df.groupby(\"mmsi\").size()\n    buques_validos = registros_por_buque[registros_por_buque >= min_registros].index\n    return df[df['mmsi'].isin(buques_validos)].copy()\n\n```\n:::\n\n\n::: {#recorte_buques .cell message='false' execution_count=35}\n``` {.python .cell-code}\ntrain_val_filtrado = filtrar_buques_por_min_registros(train_val_target, min_registros=seq_len)\nprint(f\"Filas después del recorte por secuencia mínima: {len(train_val_filtrado)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFilas después del recorte por secuencia mínima: 19140\n```\n:::\n:::\n\n\n## valores nulos\n\n::: {#valores_nulos .cell message='false' execution_count=36}\n``` {.python .cell-code}\nnulos_por_col = train_val_filtrado.isnull().sum()\nnulos_filtrados = nulos_por_col[nulos_por_col > 0]\n\nresumen_nulos = pd.DataFrame({\n    'Columna': nulos_filtrados.index,\n    'Valores nulos': nulos_filtrados.values,\n    'Porcentaje nulos (%)': nulos_filtrados / len(train_val_filtrado) * 100\n}).sort_values(by='Porcentaje nulos (%)', ascending=False)\n\nprint(tabulate(resumen_nulos, headers='keys', tablefmt='grid', showindex=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+---------------------+-----------------+------------------------+\n| Columna             |   Valores nulos |   Porcentaje nulos (%) |\n+=====================+=================+========================+\n| TotalBunkerCapacity |            3205 |             16.745     |\n+---------------------+-----------------+------------------------+\n| MainEngineRPM       |             161 |              0.84117   |\n+---------------------+-----------------+------------------------+\n| FuelType1Capacity   |              96 |              0.501567  |\n+---------------------+-----------------+------------------------+\n| ais_loa             |               4 |              0.0208986 |\n+---------------------+-----------------+------------------------+\n```\n:::\n:::\n\n\n::: {#filas_train_val_filtrado .cell message='false' execution_count=37}\n``` {.python .cell-code}\nfrom tabulate import tabulate\n\n# Crear un pequeño DataFrame resumen\nresumen_filas = pd.DataFrame({\n    'Dataset': ['train_val_filtrado'],\n    'Filas totales': [len(train_val_filtrado)]\n})\n\n# Imprimir en formato tabulate\nprint(tabulate(resumen_filas, headers='keys', tablefmt='psql', showindex=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------+-----------------+\n| Dataset            |   Filas totales |\n|--------------------+-----------------|\n| train_val_filtrado |           19140 |\n+--------------------+-----------------+\n```\n:::\n:::\n\n\nDespués de aplicar los filtros sobre el conjunto de datos, se eliminarán los registros con valores nulos en la variable objetivo CO2_emission y aquellos donde esta sea igual a cero. Tras estas operaciones, el dataset train_val_filtrado contendrá 19,140 filas, correspondientes a los buques que cumplen con los criterios mínimos de secuencia. Esto permitirá que los análisis y modelos posteriores se enfoquen únicamente en datos válidos y consistentes.\n\n\n\n\n## Partición interna\n\n::: {#particion_interna .cell message='false' execution_count=38}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\n# Definir X (features) y y (target)\nX = train_val_filtrado.drop(columns=[target])\ny = train_val_filtrado[target]\n\n# Partición interna: entrenamiento interno y validación interna\nX_train_int, X_val_int, y_train_int, y_val_int = train_test_split(\n    X, y,\n    test_size=0.2,      # 20% para validación interna\n    random_state=42,    # para reproducibilidad\n    shuffle=True\n)\n\nprint(f\"Tamaño X_train_int: {X_train_int.shape}\")\nprint(f\"Tamaño X_val_int: {X_val_int.shape}\")\nprint(f\"Tamaño y_train_int: {y_train_int.shape}\")\nprint(f\"Tamaño y_val_int: {y_val_int.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTamaño X_train_int: (15312, 50)\nTamaño X_val_int: (3828, 50)\nTamaño y_train_int: (15312,)\nTamaño y_val_int: (3828,)\n```\n:::\n:::\n\n\n###  Definición de la función de imputación\n\n::: {#imputacion_def .cell message='false' execution_count=39}\n``` {.python .cell-code}\ndef imputar_datos(df, columnas_numericas, columnas_categoricas, flags_nulos=None, medianas=None, modas=None):\n    \"\"\"\n    Imputa valores faltantes en un DataFrame.\n    \n    Args:\n        df (pd.DataFrame): DataFrame a imputar.\n        columnas_numericas (list): Columnas numéricas a imputar con mediana.\n        columnas_categoricas (list): Columnas categóricas a imputar con moda.\n        flags_nulos (list, optional): Columnas para crear flag de valores nulos originales.\n        medianas (dict, optional): Median values para usar (train stats).\n        modas (dict, optional): Moda values para usar (train stats).\n        \n    Returns:\n        df_imputado (pd.DataFrame): DataFrame imputado.\n        medianas (dict): Median values calculadas si no se pasan.\n        modas (dict): Moda values calculadas si no se pasan.\n    \"\"\"\n    df_imputado = df.copy()\n    \n    # Crear flags de nulos\n    if flags_nulos:\n        for col in flags_nulos:\n            df_imputado[f\"{col}_is_missing\"] = df_imputado[col].isna().astype(int)\n    \n    # Si no vienen medianas y modas, calcular a partir del df actual\n    if medianas is None:\n        medianas = {col: df_imputado[col].median() for col in columnas_numericas}\n    if modas is None:\n        modas = {col: df_imputado[col].mode()[0] for col in columnas_categoricas}\n    \n    # Imputar columnas numéricas\n    for col in columnas_numericas:\n        df_imputado[col] = df_imputado[col].fillna(medianas[col])\n    \n    # Imputar columnas categóricas\n    for col in columnas_categoricas:\n        df_imputado[col] = df_imputado[col].fillna(modas[col])\n    \n    return df_imputado, medianas, modas\n```\n:::\n\n\n###  Aplicación en training (entrenamiento interno)\n\n::: {#imputacion_columnas .cell message='false' execution_count=40}\n``` {.python .cell-code}\ncolumnas_num = ['TotalBunkerCapacity', 'MainEngineRPM', 'FuelType1Capacity', 'ais_loa']\nflags = ['TotalBunkerCapacity']  # opcional si quieres marcar dónde estaban los nulos\ncolumnas_cat = []  # No hay columnas categóricas con nulos\n```\n:::\n\n\n::: {#imputacion_train .cell message='false' execution_count=41}\n``` {.python .cell-code}\nX_train_int_imputado, medianas, modas = imputar_datos(\n    X_train_int,\n    columnas_numericas=columnas_num,\n    columnas_categoricas=columnas_cat,\n    flags_nulos=flags\n)\n```\n:::\n\n\n###  Aplicación en validación \n\n::: {#imputacion_val .cell message='false' execution_count=42}\n``` {.python .cell-code}\nX_val_int_imputado, _, _ = imputar_datos(\n    X_val_int,\n    columnas_numericas=columnas_num,\n    columnas_categoricas=columnas_cat,\n    flags_nulos=flags,\n    medianas=medianas,\n    modas=modas\n)\n```\n:::\n\n\n##  creación de features (feature engineering)\n\n::: {#crear_features_def .cell message='false' execution_count=43}\n``` {.python .cell-code}\ndef crear_features(df):\n    \"\"\"\n    Crea nuevas variables/features a partir de un DataFrame imputado.\n    \n    Variables creadas:\n        - duration_min: duración entre posiciones consecutivas por buque\n        - viaje_puerto: identificador de viaje basado en cambio de puerto\n        - viaje_gap: identificador de viaje basado en gaps de tiempo\n        - viaje_leg: identificador de viaje basado en start_leg y end_leg\n        - frecuencia_puerto: número de veces que un puerto aparece como port_after por buque\n    \n    Args:\n        df (pd.DataFrame): DataFrame imputado con columnas necesarias (fechas, puertos, leg info)\n        \n    Returns:\n        pd.DataFrame: DataFrame con las nuevas features\n    \"\"\"\n    df = df.copy()\n    \n    # Ordenar por buque y tiempo\n    df = df.sort_values(by=['mmsi','first_dt_pos_utc']).reset_index(drop=True)\n    \n    # duration_min\n    df['duration_min'] = df.groupby('mmsi')['first_dt_pos_utc'].diff().dt.total_seconds() / 60\n    df['duration_min'] = df['duration_min'].fillna(0)\n    \n    # viaje_puerto\n    df['viaje_change_puerto'] = (\n        (df['port_before'] != df.groupby('mmsi')['port_before'].shift(1)) |\n        (df['port_after'] != df.groupby('mmsi')['port_after'].shift(1))\n    )\n    df['viaje_puerto'] = df.groupby('mmsi')['viaje_change_puerto'].cumsum()\n    df.drop(columns=['viaje_change_puerto'], inplace=True)\n    \n    # viaje_gap\n    umbral_gap = 60\n    df['viaje_change_gap'] = (df['duration_min'] > umbral_gap).astype(int)\n    df['viaje_gap'] = df.groupby('mmsi')['viaje_change_gap'].cumsum()\n    df.drop(columns=['viaje_change_gap'], inplace=True)\n    \n    # viaje_leg\n    df['viaje_leg'] = ((df['start_leg'] != df.groupby('mmsi')['start_leg'].shift(1)) |\n                       (df['end_leg'] != df.groupby('mmsi')['end_leg'].shift(1))).cumsum()\n    \n    # frecuencia_puerto\n    freq_puerto = df.groupby(['mmsi','port_after']).size().rename('frecuencia_puerto')\n    df = df.merge(freq_puerto, how='left', on=['mmsi','port_after'])\n    \n    return df\n```\n:::\n\n\n### Aplicación a entrenamiento interno\n\n::: {#crear_features_train .cell message='false' execution_count=44}\n``` {.python .cell-code}\nX_train_int_features = crear_features(X_train_int_imputado)\n```\n:::\n\n\n### Aplicación a validación interna\n\n::: {#crear_features_val .cell message='false' execution_count=45}\n``` {.python .cell-code}\nX_val_int_features = crear_features(X_val_int_imputado)\n```\n:::\n\n\n### vista\n\n::: {#resumen_variables .cell message='false' execution_count=46}\n``` {.python .cell-code}\nfrom tabulate import tabulate\n\n# DataFrame de ejemplo: X_train_int_imputado\ndf = X_train_int_imputado.copy()\n\n# Crear lista de variables y su tipo\nvariable_tipo = [(col, str(df[col].dtype)) for col in df.columns]\n\n# Mostrar en formato tabulate\nprint(tabulate(variable_tipo, headers=['Variable', 'Tipo'], tablefmt='grid'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------------------+---------------------+\n| Variable                       | Tipo                |\n+================================+=====================+\n| mmsi                           | int32               |\n+--------------------------------+---------------------+\n| start_leg                      | datetime64[ns, UTC] |\n+--------------------------------+---------------------+\n| end_leg                        | datetime64[ns, UTC] |\n+--------------------------------+---------------------+\n| h3_sequence                    | object              |\n+--------------------------------+---------------------+\n| imo_x                          | int32               |\n+--------------------------------+---------------------+\n| ref_in                         | datetime64[us]      |\n+--------------------------------+---------------------+\n| ref_out                        | datetime64[us]      |\n+--------------------------------+---------------------+\n| port_before                    | object              |\n+--------------------------------+---------------------+\n| country_before                 | object              |\n+--------------------------------+---------------------+\n| port_after                     | object              |\n+--------------------------------+---------------------+\n| country_after                  | object              |\n+--------------------------------+---------------------+\n| op_phase                       | object              |\n+--------------------------------+---------------------+\n| StandardVesselType_x           | object              |\n+--------------------------------+---------------------+\n| GrossTonnage_x                 | int32               |\n+--------------------------------+---------------------+\n| first_dt_pos_utc               | datetime64[us]      |\n+--------------------------------+---------------------+\n| sum_me_ene                     | float64             |\n+--------------------------------+---------------------+\n| sum_ae_ene                     | float64             |\n+--------------------------------+---------------------+\n| sum_ab_ene                     | float64             |\n+--------------------------------+---------------------+\n| imo_y                          | int32               |\n+--------------------------------+---------------------+\n| GrossTonnage_y                 | int32               |\n+--------------------------------+---------------------+\n| Deadweight                     | int32               |\n+--------------------------------+---------------------+\n| LengthOverallLOA               | float64             |\n+--------------------------------+---------------------+\n| DateOfBuild                    | int32               |\n+--------------------------------+---------------------+\n| TEU                            | int32               |\n+--------------------------------+---------------------+\n| Powerkwmax                     | float64             |\n+--------------------------------+---------------------+\n| MainEngineModel                | object              |\n+--------------------------------+---------------------+\n| Speed                          | float64             |\n+--------------------------------+---------------------+\n| Speedmax                       | float64             |\n+--------------------------------+---------------------+\n| Speedservice                   | float64             |\n+--------------------------------+---------------------+\n| BreadthExtreme                 | float64             |\n+--------------------------------+---------------------+\n| SummerDraught                  | float64             |\n+--------------------------------+---------------------+\n| FuelType1Capacity              | float64             |\n+--------------------------------+---------------------+\n| FuelType2Capacity              | float64             |\n+--------------------------------+---------------------+\n| LightDisplacementTonnage       | int32               |\n+--------------------------------+---------------------+\n| MainEngineRPM                  | float64             |\n+--------------------------------+---------------------+\n| MainEngineType                 | object              |\n+--------------------------------+---------------------+\n| Powerkwservice                 | int32               |\n+--------------------------------+---------------------+\n| PropulsionType                 | object              |\n+--------------------------------+---------------------+\n| ShiptypeLevel5                 | object              |\n+--------------------------------+---------------------+\n| TotalBunkerCapacity            | float64             |\n+--------------------------------+---------------------+\n| StandardVesselType_y           | object              |\n+--------------------------------+---------------------+\n| imobin                         | int64               |\n+--------------------------------+---------------------+\n| fuel                           | object              |\n+--------------------------------+---------------------+\n| meType                         | object              |\n+--------------------------------+---------------------+\n| ais_beam                       | float64             |\n+--------------------------------+---------------------+\n| ais_loa                        | float64             |\n+--------------------------------+---------------------+\n| combustible_me                 | float64             |\n+--------------------------------+---------------------+\n| combustible_ae                 | float64             |\n+--------------------------------+---------------------+\n| combustible_ab                 | float64             |\n+--------------------------------+---------------------+\n| fuel_consumption               | float64             |\n+--------------------------------+---------------------+\n| TotalBunkerCapacity_is_missing | int64               |\n+--------------------------------+---------------------+\n```\n:::\n:::\n\n\n##  Visualización de matriz de correlación (heatmap)\n\n::: {#visualizar_correlacion .cell message='false' execution_count=47}\n``` {.python .cell-code}\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef mostrar_matriz_correlacion(df, figsize=(12,10), cmap='coolwarm'):\n    \"\"\"\n    Genera un heatmap de la matriz de correlación solo para columnas numéricas.\n    \n    Args:\n        df (pd.DataFrame): DataFrame a analizar.\n        figsize (tuple, optional): Tamaño de la figura. Default (12,10).\n        cmap (str, optional): Colormap de Seaborn. Default 'coolwarm'.\n        \n    Retorna:\n        corr (pd.DataFrame): Matriz de correlación (solo numéricas) para uso posterior.\n    \"\"\"\n    df_numerico = df.select_dtypes(include='number')\n    corr = df_numerico.corr()\n    \n    plt.figure(figsize=figsize)\n    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=cmap, cbar=True, square=True)\n    plt.title(\"Matriz de correlación (columnas numéricas)\")\n    plt.show()\n    \n    return corr\n\n```\n:::\n\n\n::: {#a8b887f7 .cell execution_count=48}\n``` {.python .cell-code}\n# Instancia: aplicar a X_train_int_imputado\ncorr_train = mostrar_matriz_correlacion(X_train_int_imputado)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-49-output-1.png){}\n:::\n:::\n\n\n### **Definición de la función para eliminar correlación alta**\n\n::: {#eliminar_correlacion .cell message='false' execution_count=49}\n``` {.python .cell-code}\nimport numpy as np\n\n# Función para elegir variables altamente correlacionadas\ndef choose_one_from_highly_correlated(correlation_matrix, threshold=0.60):\n    \"\"\"\n    Identifica columnas a eliminar según correlación alta.\n    \n    Args:\n        correlation_matrix (pd.DataFrame): matriz de correlación absoluta.\n        threshold (float): umbral de correlación para eliminar variables.\n    \n    Returns:\n        list: columnas a eliminar.\n    \"\"\"\n    columns_to_remove = set()\n    for column in correlation_matrix.columns:\n        high_corr_columns = correlation_matrix[column][correlation_matrix[column] > threshold].index.tolist()\n        if column in high_corr_columns:\n            high_corr_columns.remove(column)\n        if high_corr_columns:\n            columns_to_remove.update(high_corr_columns[1:])\n    return list(columns_to_remove)\n```\n:::\n\n\n### **Aplicación de la función a entrenamiento y validación**\n\n::: {#aplicar_eliminar_correlacion .cell message='false' execution_count=50}\n``` {.python .cell-code}\nimport numpy as np\n\n# Calcular matriz de correlación absoluta solo de columnas numéricas\ncorrelation_matrix_train = X_train_int_imputado.select_dtypes(include=[np.number]).corr().abs()\n\n# Identificar columnas a eliminar según correlación\ncolumns_to_drop = choose_one_from_highly_correlated(correlation_matrix_train, threshold=0.60)\n\n# Eliminar columnas de X_train_int y X_val_int\nX_train_int_sin_corr = X_train_int_imputado.drop(columns=columns_to_drop)\nX_val_int_sin_corr = X_val_int_imputado.drop(columns=columns_to_drop)\n\n# Resumen\nprint(f\"Columnas originales en train: {X_train_int_imputado.shape[1]}\")\nprint(f\"Columnas eliminadas por alta correlación: {len(columns_to_drop)}\")\nprint(f\"Columnas después de eliminación en train: {X_train_int_sin_corr.shape[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColumnas originales en train: 51\nColumnas eliminadas por alta correlación: 18\nColumnas después de eliminación en train: 33\n```\n:::\n:::\n\n\n# Aplicar la eliminación de columnas seleccionadas a train y validación\n\n::: {#eliminar_variables .cell message='false' execution_count=51}\n``` {.python .cell-code}\n# eliminar de train y validación usando la lista calculada\nX_train_int_imputado = X_train_int_imputado.drop(columns=columns_to_drop)\nX_val_int_imputado   = X_val_int_imputado.drop(columns=columns_to_drop)\n```\n:::\n\n\n## variables resultante\n\n::: {#b3b3af0b .cell execution_count=52}\n``` {.python .cell-code}\nfrom tabulate import tabulate\n\n# Crear lista de columnas y su tipo\nvariable_tipo = [(col, str(X_train_int_imputado[col].dtype)) for col in X_train_int_imputado.columns]\n\n# Mostrar en formato tabla\nprint(tabulate(variable_tipo, headers=['Variable', 'Tipo'], tablefmt='grid'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------------------+---------------------+\n| Variable                       | Tipo                |\n+================================+=====================+\n| mmsi                           | int32               |\n+--------------------------------+---------------------+\n| start_leg                      | datetime64[ns, UTC] |\n+--------------------------------+---------------------+\n| end_leg                        | datetime64[ns, UTC] |\n+--------------------------------+---------------------+\n| h3_sequence                    | object              |\n+--------------------------------+---------------------+\n| imo_x                          | int32               |\n+--------------------------------+---------------------+\n| ref_in                         | datetime64[us]      |\n+--------------------------------+---------------------+\n| ref_out                        | datetime64[us]      |\n+--------------------------------+---------------------+\n| port_before                    | object              |\n+--------------------------------+---------------------+\n| country_before                 | object              |\n+--------------------------------+---------------------+\n| port_after                     | object              |\n+--------------------------------+---------------------+\n| country_after                  | object              |\n+--------------------------------+---------------------+\n| op_phase                       | object              |\n+--------------------------------+---------------------+\n| StandardVesselType_x           | object              |\n+--------------------------------+---------------------+\n| GrossTonnage_x                 | int32               |\n+--------------------------------+---------------------+\n| first_dt_pos_utc               | datetime64[us]      |\n+--------------------------------+---------------------+\n| sum_me_ene                     | float64             |\n+--------------------------------+---------------------+\n| sum_ab_ene                     | float64             |\n+--------------------------------+---------------------+\n| imo_y                          | int32               |\n+--------------------------------+---------------------+\n| DateOfBuild                    | int32               |\n+--------------------------------+---------------------+\n| MainEngineModel                | object              |\n+--------------------------------+---------------------+\n| Speedmax                       | float64             |\n+--------------------------------+---------------------+\n| BreadthExtreme                 | float64             |\n+--------------------------------+---------------------+\n| FuelType1Capacity              | float64             |\n+--------------------------------+---------------------+\n| MainEngineRPM                  | float64             |\n+--------------------------------+---------------------+\n| MainEngineType                 | object              |\n+--------------------------------+---------------------+\n| Powerkwservice                 | int32               |\n+--------------------------------+---------------------+\n| PropulsionType                 | object              |\n+--------------------------------+---------------------+\n| ShiptypeLevel5                 | object              |\n+--------------------------------+---------------------+\n| StandardVesselType_y           | object              |\n+--------------------------------+---------------------+\n| fuel                           | object              |\n+--------------------------------+---------------------+\n| meType                         | object              |\n+--------------------------------+---------------------+\n| combustible_ab                 | float64             |\n+--------------------------------+---------------------+\n| TotalBunkerCapacity_is_missing | int64               |\n+--------------------------------+---------------------+\n```\n:::\n:::\n\n\n## outliers\n\n\n\n## **Definir features numéricas y categóricas**\n\n::: {#definir_features .cell message='false' execution_count=53}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Columnas numéricas que vamos a escalar\n\nnumeric_features = [\n    'GrossTonnage_x', 'sum_me_ene', 'sum_ab_ene', \n    'Speedmax', 'BreadthExtreme', 'FuelType1Capacity', \n    'MainEngineRPM', 'Powerkwservice', 'combustible_ab', \n    'TotalBunkerCapacity_is_missing'\n]\n\n\n# Columnas categóricas (objetos) excluyendo h3_sequence\ncategorical_features = X_train_int_imputado.select_dtypes(include=['object']).columns.tolist()\nif 'h3_sequence' in categorical_features:\n    categorical_features.remove('h3_sequence')\n\nprint(\"Numéricas:\", numeric_features)\nprint(\"Categóricas:\", categorical_features)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNuméricas: ['GrossTonnage_x', 'sum_me_ene', 'sum_ab_ene', 'Speedmax', 'BreadthExtreme', 'FuelType1Capacity', 'MainEngineRPM', 'Powerkwservice', 'combustible_ab', 'TotalBunkerCapacity_is_missing']\nCategóricas: ['port_before', 'country_before', 'port_after', 'country_after', 'op_phase', 'StandardVesselType_x', 'MainEngineModel', 'MainEngineType', 'PropulsionType', 'ShiptypeLevel5', 'StandardVesselType_y', 'fuel', 'meType']\n```\n:::\n:::\n\n\n## **Escalado de features numéricas y codificación de categóricas**\n\n::: {#escalado_numerico .cell message='false' execution_count=54}\n``` {.python .cell-code}\n# Escalado numérico\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = X_train_int_imputado.copy()\nX_val_scaled = X_val_int_imputado.copy()\n\nX_train_scaled[numeric_features] = scaler.fit_transform(X_train_scaled[numeric_features])\nX_val_scaled[numeric_features] = scaler.transform(X_val_scaled[numeric_features])\n\nprint(\"Escalado numérico realizado ✅\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEscalado numérico realizado ✅\n```\n:::\n:::\n\n\n::: {#conversion_categoricas .cell message='false' execution_count=55}\n``` {.python .cell-code}\n# Convertir todas las columnas categóricas a string (por si hay tipos mixtos)\nX_train_scaled[categorical_features] = X_train_scaled[categorical_features].astype(str)\nX_val_scaled[categorical_features] = X_val_scaled[categorical_features].astype(str)\n\nprint(\"Columnas categóricas convertidas a string ✅\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColumnas categóricas convertidas a string ✅\n```\n:::\n:::\n\n\n::: {#codificacion_onehot .cell message='false' execution_count=56}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Codificación one-hot\nencoder = ColumnTransformer(\n    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],\n    remainder='drop'  # solo se procesan las columnas categóricas que definimos\n)\n\nX_train_enc = encoder.fit_transform(X_train_scaled)\nX_val_enc = encoder.transform(X_val_scaled)\n\nprint(\"Codificación One-Hot realizada ✅\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCodificación One-Hot realizada ✅\n```\n:::\n:::\n\n\n::: {#conversion_dataframe .cell message='false' execution_count=57}\n``` {.python .cell-code}\n# Convertimos a DataFrame para inspección\nimport pandas as pd\n\nX_train_df = pd.DataFrame(X_train_enc.toarray() if hasattr(X_train_enc, \"toarray\") else X_train_enc)\nX_val_df = pd.DataFrame(X_val_enc.toarray() if hasattr(X_val_enc, \"toarray\") else X_val_enc)\n\nprint(\"Conversión a DataFrame realizada ✅\")\nprint(\"Dimensiones X_train_df:\", X_train_df.shape)\nprint(\"Dimensiones X_val_df:\", X_val_df.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConversión a DataFrame realizada ✅\nDimensiones X_train_df: (15312, 1364)\nDimensiones X_val_df: (3828, 1364)\n```\n:::\n:::\n\n\n::: {#verificacion_targuet .cell message='false' execution_count=58}\n``` {.python .cell-code}\nprint(\"NaN en y_train_int:\", y_train_int.isna().sum())\nprint(\"NaN en y_val_int:\", y_val_int.isna().sum())\nprint(\"Valores únicos en y_train_int:\", y_train_int.unique())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNaN en y_train_int: 0\nNaN en y_val_int: 0\nValores únicos en y_train_int: [2.47487183e+03 2.08071100e+01 7.57015043e+02 ... 1.39331007e+03\n 1.60673185e+04 3.65883159e+04]\n```\n:::\n:::\n\n\n## **Escalado de la variable objetivo**\n\n::: {#escalado_target .cell message='false' execution_count=59}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\nscaler_y = MinMaxScaler()\ny_train_scaled = scaler_y.fit_transform(y_train_int.values.reshape(-1,1))\ny_val_scaled = scaler_y.transform(y_val_int.values.reshape(-1,1))\n```\n:::\n\n\n## inspeccionar_target\n\n::: {#verificar_y .cell message='false' execution_count=60}\n``` {.python .cell-code}\nimport numpy as np\n\nprint(\"NaN en y_train_scaled:\", np.isnan(y_train_scaled).sum())\nprint(\"Inf en y_train_scaled:\", np.isinf(y_train_scaled).sum())\nprint(\"NaN en y_val_scaled:\", np.isnan(y_val_scaled).sum())\nprint(\"Inf en y_val_scaled:\", np.isinf(y_val_scaled).sum())\n\nprint(\"Valores mínimos y máximos en y_train_scaled:\", y_train_scaled.min(), y_train_scaled.max())\nprint(\"Valores mínimos y máximos en y_val_scaled:\", y_val_scaled.min(), y_val_scaled.max())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNaN en y_train_scaled: 0\nInf en y_train_scaled: 0\nNaN en y_val_scaled: 0\nInf en y_val_scaled: 0\nValores mínimos y máximos en y_train_scaled: 0.0 1.0000000000000002\nValores mínimos y máximos en y_val_scaled: -5.700627960199813e-09 1.2468833255578058\n```\n:::\n:::\n\n\n## **Crear secuencias para LSTM (con índices originales)**\n\n::: {#crear_secuencias .cell message='false' execution_count=61}\n``` {.python .cell-code}\nseq_len = 3\n\ndef create_sequences(X, y, df_indices, seq_len=3):\n    X_seq, y_seq, idx_seq_all = [], [], []\n    for _, group in df_indices.groupby('mmsi'):\n        n = len(group)\n        if n <= seq_len:\n            continue\n        for i in range(n - seq_len):\n            idx_seq = group.index[i:i+seq_len].to_list()\n            X_seq.append(X[idx_seq])\n            y_seq.append(y[idx_seq[-1]])\n            # Guardamos el índice real del último valor de la secuencia\n            idx_seq_all.append(group.loc[idx_seq[-1], \"index_original\"])\n    return np.array(X_seq), np.array(y_seq), np.array(idx_seq_all)\n\n# Guardar índice original\nX_train_scaled_reset = X_train_scaled.reset_index(drop=True)\nX_train_scaled_reset['index_original'] = X_train_scaled_reset.index\n\nX_val_scaled_reset = X_val_scaled.reset_index(drop=True)\nX_val_scaled_reset['index_original'] = X_val_scaled_reset.index\n\n# Crear secuencias\nX_train_seq, y_train_seq, idx_train_seq = create_sequences(\n    X_train_df.values, y_train_scaled, X_train_scaled_reset, seq_len\n)\nX_val_seq, y_val_seq, idx_val_seq = create_sequences(\n    X_val_df.values, y_val_scaled, X_val_scaled_reset, seq_len\n)\n```\n:::\n\n\n## **Construcción y entrenamiento del modelo LSTM**\n\n::: {#entrenamiento_lstm .cell message='false' execution_count=62}\n``` {.python .cell-code}\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n\nmodel = Sequential()\nmodel.add(Input(shape=(seq_len, X_train_seq.shape[2])))\nmodel.add(LSTM(64, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\nhistory = model.fit(\n    X_train_seq, y_train_seq,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_val_seq, y_val_seq),\n    verbose=1\n)\n```\n:::\n\n\n## **Evaluación del modelo**\n\n::: {#evaluacion_lstm_metricas .cell message='false' execution_count=63}\n``` {.python .cell-code}\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# Evaluación del modelo en validación\nloss, mae = model.evaluate(X_val_seq, y_val_seq, verbose=0)\n\n# Predicciones\ny_pred_scaled = model.predict(X_val_seq, verbose=0)\ny_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))\ny_true = scaler_y.inverse_transform(y_val_seq.reshape(-1, 1))\n\n# Métricas\nmae_original = np.mean(np.abs(y_true - y_pred))\nr2 = r2_score(y_true, y_pred)\nepsilon = 1e-8\nmape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n\n# Prints resumidos y claros\nprint(f\"R²: {r2:.4f}\")\nprint(f\"MAPE: {mape:.2f}%\")\nprint(f\"Loss (scaled): {loss:.4f}\")\nprint(f\"MAE (scaled): {mae:.4f}\")\nprint(f\"MAE original: {mae_original:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR²: 0.7696\nMAPE: 63968.24%\nLoss (scaled): 0.0003\nMAE (scaled): 0.0057\nMAE original: 150875.57\n```\n:::\n:::\n\n\n::: {#cell-tu_loss .cell message='false' execution_count=64}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Supongamos que tu entrenamiento devuelve un objeto 'history'\n# history.history['loss'] contiene el loss por época\nloss = history.history['loss']  \nval_loss = history.history.get('val_loss')  # opcional, si tienes validación\n\nplt.figure(figsize=(8,5))\nplt.plot(loss, label='Entrenamiento')\nif val_loss:\n    plt.plot(val_loss, label='Validación')\nplt.title('Evolución del Loss')\nplt.xlabel('Época')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/tu_loss-output-1.png){#tu_loss}\n:::\n:::\n\n\n## conjunto de prueba\n\n\n\n##  Crear target CO2\\_emission en test\n\n::: {#crear_target_test .cell message='false' execution_count=65}\n``` {.python .cell-code}\n# Crear target en test_ext\ntest_ext = crear_target_emisiones(test_ext)\ntarget = 'CO2_emission'\n```\n:::\n\n\n::: {#08ca055d .cell message='false' execution_count=66}\n``` {.python .cell-code}\ntest_ext_filtrado = test_ext.copy()\n```\n:::\n\n\n##  Eliminar filas con CO2\\_emission = 0\n\n::: {#db70da1d .cell message='false' execution_count=67}\n``` {.python .cell-code}\ntest_ext_filtrado = test_ext_filtrado[test_ext_filtrado[target] != 0].copy()\n```\n:::\n\n\n---\n\n##  Revisar cuántas filas quedan tras eliminar ceros\n\n::: {#f469b155 .cell message='false' execution_count=68}\n``` {.python .cell-code}\nprint(f\"Filas en test después de eliminar ceros en CO2_emission: {len(test_ext_filtrado)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFilas en test después de eliminar ceros en CO2_emission: 1887\n```\n:::\n:::\n\n\n---\n\n##  Filtrar filas con target válido y revisar buques con secuencia mínima\n\n::: {#61cbd917 .cell message='false' execution_count=69}\n``` {.python .cell-code}\nseq_len = 4  # longitud mínima de secuencia\n\n# Filtrar filas con target válido\ntest_ext_target = test_ext_filtrado[test_ext_filtrado['CO2_emission'].notna()].copy()\n\n# Contar buques válidos antes y después de filtrar NaN\nregistros_por_buque_orig = test_ext_filtrado.groupby('mmsi').size()\nbuques_validos_orig = (registros_por_buque_orig >= seq_len).sum()\n\nregistros_por_buque_filtrado = test_ext_target.groupby('mmsi').size()\nbuques_validos_filtrado = (registros_por_buque_filtrado >= seq_len).sum()\n\n# Resumen\nresumen_test = pd.DataFrame({\n    'Dataset': ['Original test', 'Filtrado por target'],\n    'Buques válidos': [buques_validos_orig, buques_validos_filtrado],\n    'Filas totales': [len(test_ext_filtrado), len(test_ext_target)]\n})\n\nfrom tabulate import tabulate\nprint(tabulate(resumen_test, headers='keys', tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---------------------+------------------+-----------------+\n|    | Dataset             |   Buques válidos |   Filas totales |\n|----+---------------------+------------------+-----------------|\n|  0 | Original test       |              312 |            1887 |\n|  1 | Filtrado por target |              312 |            1885 |\n+----+---------------------+------------------+-----------------+\n```\n:::\n:::\n\n\n---\n\n##  Recorte de buques por secuencia mínima\n\n::: {#d116e09f .cell message='false' execution_count=70}\n``` {.python .cell-code}\ntest_ext_filtrado = filtrar_buques_por_min_registros(test_ext_target, min_registros=seq_len)\nprint(f\"Filas después del recorte por secuencia mínima en test: {len(test_ext_filtrado)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFilas después del recorte por secuencia mínima en test: 1631\n```\n:::\n:::\n\n\n::: {#2e63d72b .cell message='false' execution_count=71}\n``` {.python .cell-code}\n# Ver valores nulos en test_ext_filtrado\nnulos_por_col = test_ext_filtrado.isnull().sum()\nnulos_filtrados = nulos_por_col[nulos_por_col > 0]\n\nresumen_nulos = pd.DataFrame({\n    'Columna': nulos_filtrados.index,\n    'Valores nulos': nulos_filtrados.values,\n    'Porcentaje nulos (%)': nulos_filtrados / len(test_ext_filtrado) * 100\n}).sort_values(by='Porcentaje nulos (%)', ascending=False)\n\nfrom tabulate import tabulate\nprint(tabulate(resumen_nulos, headers='keys', tablefmt='grid', showindex=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+---------------------+-----------------+------------------------+\n| Columna             |   Valores nulos |   Porcentaje nulos (%) |\n+=====================+=================+========================+\n| TotalBunkerCapacity |             310 |              19.0067   |\n+---------------------+-----------------+------------------------+\n| MainEngineRPM       |              13 |               0.797057 |\n+---------------------+-----------------+------------------------+\n| FuelType1Capacity   |               5 |               0.30656  |\n+---------------------+-----------------+------------------------+\n```\n:::\n:::\n\n\n## definir vairables de entrada \n\n::: {#0615ed8c .cell message='false' execution_count=72}\n``` {.python .cell-code}\nX_test = test_ext_filtrado.drop(columns=[target])\ny_test = test_ext_filtrado[target]\n```\n:::\n\n\n::: {#4dd80432 .cell message='false' execution_count=73}\n``` {.python .cell-code}\n# Columnas a imputar en test\ncolumnas_num = ['TotalBunkerCapacity', 'MainEngineRPM', 'FuelType1Capacity']\nflags = ['TotalBunkerCapacity']  # opcional: marcar dónde había nulos\ncolumnas_cat = []  # no hay categóricas con nulos\n\n# Aplicar imputación al test usando stats del train\nX_test_imputado, _, _ = imputar_datos(\n    X_test,\n    columnas_numericas=columnas_num,\n    columnas_categoricas=columnas_cat,\n    flags_nulos=flags,\n    medianas=medianas,  # de X_train_int_imputado\n    modas=modas\n)\n```\n:::\n\n\n## features\n\n::: {#5ea964c7 .cell message='false' execution_count=74}\n``` {.python .cell-code}\nX_test_features = crear_features(X_test_imputado)\n```\n:::\n\n\n## ajustes de correlaciones\n\n::: {#7c0fa0f4 .cell message='false' execution_count=75}\n``` {.python .cell-code}\nX_test_sin_corr = X_test_imputado.drop(columns=columns_to_drop)\n\nprint(f\"Columnas en test antes de eliminar correladas: {X_test_imputado.shape[1]}\")\nprint(f\"Columnas en test después de eliminar correladas: {X_test_sin_corr.shape[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColumnas en test antes de eliminar correladas: 51\nColumnas en test después de eliminar correladas: 33\n```\n:::\n:::\n\n\n::: {#4a4f2646 .cell message='false' execution_count=76}\n``` {.python .cell-code}\nfrom tabulate import tabulate\n\n# Resumen de columnas finales del test\nresumen_test_final = pd.DataFrame({\n    \"Columnas finales\": X_test_sin_corr.columns,\n    \"Tipo\": X_test_sin_corr.dtypes.values\n})\n\nprint(tabulate(resumen_test_final, headers=\"keys\", tablefmt=\"psql\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+--------------------------------+---------------------+\n|    | Columnas finales               | Tipo                |\n|----+--------------------------------+---------------------|\n|  0 | mmsi                           | int32               |\n|  1 | start_leg                      | datetime64[ns, UTC] |\n|  2 | end_leg                        | datetime64[ns, UTC] |\n|  3 | h3_sequence                    | object              |\n|  4 | imo_x                          | int32               |\n|  5 | ref_in                         | datetime64[us]      |\n|  6 | ref_out                        | datetime64[us]      |\n|  7 | port_before                    | object              |\n|  8 | country_before                 | object              |\n|  9 | port_after                     | object              |\n| 10 | country_after                  | object              |\n| 11 | op_phase                       | object              |\n| 12 | StandardVesselType_x           | object              |\n| 13 | GrossTonnage_x                 | int32               |\n| 14 | first_dt_pos_utc               | datetime64[us]      |\n| 15 | sum_me_ene                     | float64             |\n| 16 | sum_ab_ene                     | float64             |\n| 17 | imo_y                          | int32               |\n| 18 | DateOfBuild                    | int32               |\n| 19 | MainEngineModel                | object              |\n| 20 | Speedmax                       | float64             |\n| 21 | BreadthExtreme                 | float64             |\n| 22 | FuelType1Capacity              | float64             |\n| 23 | MainEngineRPM                  | float64             |\n| 24 | MainEngineType                 | object              |\n| 25 | Powerkwservice                 | int32               |\n| 26 | PropulsionType                 | object              |\n| 27 | ShiptypeLevel5                 | object              |\n| 28 | StandardVesselType_y           | object              |\n| 29 | fuel                           | object              |\n| 30 | meType                         | object              |\n| 31 | combustible_ab                 | float64             |\n| 32 | TotalBunkerCapacity_is_missing | int64               |\n+----+--------------------------------+---------------------+\n```\n:::\n:::\n\n\n## features test\n\n::: {#84148163 .cell message='false' execution_count=77}\n``` {.python .cell-code}\n# Features numéricas (las que se escalan)\nnumeric_features = [\n    'GrossTonnage_x', 'sum_me_ene', 'sum_ab_ene',\n    'Speedmax', 'BreadthExtreme', 'FuelType1Capacity',\n    'MainEngineRPM', 'Powerkwservice', 'combustible_ab',\n    'TotalBunkerCapacity_is_missing'\n]\n\n# Features categóricas (objetos), excluyendo 'h3_sequence'\ncategorical_features = [\n    'port_before', 'country_before', 'port_after', 'country_after',\n    'op_phase', 'StandardVesselType_x', 'MainEngineModel', \n    'MainEngineType', 'PropulsionType', 'ShiptypeLevel5',\n    'StandardVesselType_y', 'fuel', 'meType'\n]\n```\n:::\n\n\n::: {#21dacdbd .cell message='false' execution_count=78}\n``` {.python .cell-code}\n# Escalar columnas numéricas\nX_test_scaled = X_test_sin_corr.copy()\nX_test_scaled[numeric_features] = scaler.transform(X_test_scaled[numeric_features])\n\n#  Convertir categóricas a string\nX_test_scaled[categorical_features] = X_test_scaled[categorical_features].astype(str)\n\n#  Codificación One-Hot usando encoder ya ajustado\nX_test_enc = encoder.transform(X_test_scaled)\nX_test_df = pd.DataFrame(X_test_enc.toarray() if hasattr(X_test_enc, \"toarray\") else X_test_enc)\n\n```\n:::\n\n\n::: {#65986aab .cell message='false' execution_count=79}\n``` {.python .cell-code}\n#  Escalar variable objetivo\ny_test_scaled = scaler_y.transform(y_test.values.reshape(-1,1))\n\n```\n:::\n\n\n::: {#b7ac1082 .cell message='false' execution_count=80}\n``` {.python .cell-code}\n#  Crear secuencias para LSTM\nX_test_scaled_reset = X_test_scaled.reset_index(drop=True)\nX_test_scaled_reset['index_original'] = X_test_scaled_reset.index\n\nX_test_seq, y_test_seq, idx_test_seq = create_sequences(\n    X_test_df.values, y_test_scaled, X_test_scaled_reset, seq_len\n)\n\n```\n:::\n\n\n::: {#316ce5dd .cell message='false' execution_count=81}\n``` {.python .cell-code}\n#  Evaluar modelo\nloss, mae = model.evaluate(X_test_seq, y_test_seq)\nprint(\"Test Loss:\", loss, \"Test MAE:\", mae)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bold\"> 1/12</span> <span class=\"ansi-green-fg\">━</span><span class=\"ansi-white-fg\">━━━━━━━━━━━━━━━━━━━</span> <span class=\"ansi-bold\">3s</span> 275ms/step - loss: 1.4128e-04 - mae: 0.0043\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n<span class=\"ansi-bold\">12/12</span> <span class=\"ansi-green-fg\">━━━━━━━━━━━━━━━━━━━━</span> <span class=\"ansi-bold\">0s</span> 5ms/step - loss: 1.8030e-04 - mae: 0.0050  \n\nTest Loss: 0.00018030412320513278 Test MAE: 0.004961726255714893\n</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#9f0e36de .cell message='false' execution_count=82}\n``` {.python .cell-code}\n# Obtener predicciones en el test\ny_test_pred = model.predict(X_test_seq)\n\n# Reconstrucción con índices originales\ndf_results_test = pd.DataFrame({\n    \"index_original\": idx_test_seq,\n    \"y_true\": y_test_seq.flatten(),\n    \"y_pred\": y_test_pred.flatten()\n})\n\n# Ver las primeras filas\ndf_results_test.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bold\"> 1/12</span> <span class=\"ansi-green-fg\">━</span><span class=\"ansi-white-fg\">━━━━━━━━━━━━━━━━━━━</span> <span class=\"ansi-bold\">2s</span> 197ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n<span class=\"ansi-bold\">12/12</span> <span class=\"ansi-green-fg\">━━━━━━━━━━━━━━━━━━━━</span> <span class=\"ansi-bold\">0s</span> 4ms/step  \n</pre>\n```\n:::\n\n:::\n\n::: {.cell-output .cell-output-display execution_count=82}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index_original</th>\n      <th>y_true</th>\n      <th>y_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1621</td>\n      <td>0.000053</td>\n      <td>0.001364</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>791</td>\n      <td>0.000052</td>\n      <td>0.001750</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>752</td>\n      <td>0.000602</td>\n      <td>0.001954</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>260</td>\n      <td>0.000163</td>\n      <td>0.001769</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>261</td>\n      <td>0.014625</td>\n      <td>0.011861</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#2e5fa030 .cell message='false' execution_count=83}\n``` {.python .cell-code}\ndf_results_test.to_csv(\"df_results_test.csv\", index=False)\n```\n:::\n\n\n::: {#69089ba6 .cell execution_count=84}\n``` {.python .cell-code}\n# Extraer mmsi para cada predicción\ndf_results_test['mmsi'] = X_test_scaled_reset.loc[df_results_test['index_original'], 'mmsi'].values\n\n# Elegir un MMSI aleatorio del conjunto de prueba\nimport numpy as np\nmmsi_random = np.random.choice(df_results_test['mmsi'].unique())\n\n# Filtrar predicciones de ese buque\ndf_buque = df_results_test[df_results_test['mmsi'] == mmsi_random]\nprint(df_buque)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     index_original   y_true    y_pred       mmsi\n313             501  0.00025  0.002559  636016650\n```\n:::\n:::\n\n\n::: {#e7a6d0d2 .cell execution_count=85}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n\n# Contar cuántos registros tiene cada MMSI\nconteo_buques = df_results_test['mmsi'].value_counts()\n\n# Elegir un MMSI con al menos, por ejemplo, 5 secuencias\nmmsi_ejemplo = conteo_buques[conteo_buques >= 5].index[0]\n\n# Filtrar predicciones de ese buque\ndf_buque = df_results_test[df_results_test['mmsi'] == mmsi_ejemplo]\n\n# Graficar\nplt.figure(figsize=(12,6))\nplt.plot(df_buque['y_true'].values, label='y_true', marker='o')\nplt.plot(df_buque['y_pred'].values, label='y_pred', marker='x')\nplt.title(f'Predicciones vs Valores reales para MMSI {mmsi_ejemplo}')\nplt.xlabel('Secuencia')\nplt.ylabel('CO2_emission (escalado)')\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-86-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}