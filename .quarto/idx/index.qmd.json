{"title":"Green Maritime Corridors - Canal de Panamá","markdown":{"headingText":"Green Maritime Corridors - Canal de Panamá","containsRefs":false,"markdown":"\n\nquarto preview\n\n## Librerías y dependencias\n\n```{python}\n#| label: librerias\n#| warning: false\n#| message: false\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_selection import SelectKBest, f_regression\n```\n\n\n## Lectura y limpieza inicial\n\n```{python}\n#| label: lectura_limpieza_inicial\n# Lectura y limpieza inicial\nruta_data = r\"C:\\Users\\limco\\Documents\\DATA_C\\sample_training.parquet\"\ndf_raw = pd.read_parquet(ruta_data, engine='fastparquet')\n\n# Limpiar nombres de columnas\ndf_raw.columns = (\n    df_raw.columns.str.strip()\n                  .str.replace(' ', '_')\n                  .str.replace(r'[^\\w]', '', regex=True)\n)\n\n# Convertir columna de fecha\ndf_raw['first_dt_pos_utc'] = pd.to_datetime(df_raw['first_dt_pos_utc'])\n```\n\n\n\n## Partición externa\n\n```{python}\n#| label: particion_externa\n# Partición externa\nfecha_corte = pd.to_datetime('2019-06-01')\ntrain_val = df_raw[df_raw['first_dt_pos_utc'] < fecha_corte].copy()\ntest_ext  = df_raw[df_raw['first_dt_pos_utc'] >= fecha_corte].copy()\n```\n\n\n# Exploración inicial (EDA)\n\n## Resumen de train\\_val\n\n```{python}\n#| label: resumen_train_val\n#| warning: false\n#| message: false\nfrom tabulate import tabulate\n\nprint(f\"Cantidad de filas en train_val: {len(train_val)}\")\nprint(f\"Cantidad de columnas en train_val: {len(train_val.columns)}\\n\")\n```\n\n\n\n## Exploración inicial de datos (sin h3\\_sequence)\n\n```{python}\n#| label: exploracion_i\n#| warning: false\n#| message: false\nfrom tabulate import tabulate\n\n# Columnas a mostrar (excluyendo 'h3_sequence')\ncols_to_show = [c for c in train_val.columns if c != 'h3_sequence']\n\n# Mostrar primeras filas\nprint(tabulate(train_val[cols_to_show].head(5), headers='keys', tablefmt='psql'))\n```\n\n\n\n## Revisión de tipos de datos\n\n```{python}\n#| label: tipos_columnas\n#| warning: false\n#| message: false\nfrom tabulate import tabulate\n\ntipos = train_val.dtypes.reset_index()\ntipos.columns = ['Columna', 'Tipo de dato']\n\nprint(tabulate(tipos, headers='keys', tablefmt='psql'))\n```\n\n\n\n## Distribución de registros por buque\n\n```{python}\n#| label: eda_secuencias\n#| warning: false\n#| message: false\nfrom tabulate import tabulate\n\nregistros_por_buque = train_val.groupby(\"mmsi\").size()\ntabla_freq = registros_por_buque.value_counts().sort_index()\ntabla_freq_df = tabla_freq.reset_index()\ntabla_freq_df.columns = [\"n_registros_por_buque\", \"n_buques\"]\ntabla_freq_df = tabla_freq_df.sort_values(by=\"n_buques\", ascending=False).reset_index(drop=True)\n```\n\n\nLa tabla muestra que la mayoría de los buques tienen pocos registros, la mayor parte entre 1 y 5 pasos. Solo unos pocos buques tienen muchos registros (más de 50), lo que indica que unos pocos son muy activos mientras la mayoría realiza trayectorias cortas o tiene menos datos registrados\n\n\n\n```{python}\n#| label: a_secuencias\n#| warning: false\n#| message: false\nprint(\"\\nTabla de registros por buque ordenada por n_buques:\")\nprint(tabulate(tabla_freq_df, headers='keys', tablefmt='psql'))\n```\n\n\n\n### Gráfico de distribución de registros por buque\n\n```{python}\n#| label: eda_grafico\n#| warning: false\n#| message: false\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntabla_freq_df['n_buques_acum'] = tabla_freq_df['n_buques'].cumsum()\n\nplt.figure(figsize=(12,5))\nsns.barplot(\n    data=tabla_freq_df, \n    x='n_registros_por_buque', \n    y='n_buques', \n    color='steelblue'\n)\nplt.xlabel(\"Número de registros por buque\")\nplt.ylabel(\"Número de buques\")\nplt.title(\"Distribución de registros por buque (train_val)\")\nplt.xticks(rotation=90)\nplt.show()\n```\n\n\n\n## Verificar viajes puerto–puerto\n\n```{python}\n#| label: eda_viajes_puerto\n#| warning: false\n#| message: false\nviajes_incompletos = train_val[\n    train_val['port_before'].isna() | \n    train_val['port_after'].isna()  |\n    (train_val['port_before'].str.lower().isin(['', 'desconocido'])) |\n    (train_val['port_after'].str.lower().isin(['', 'desconocido']))\n]\n\nprint(\"Cantidad de registros con viajes incompletos (puertos faltantes o desconocidos):\", len(viajes_incompletos))\nprint(tabulate(\n    viajes_incompletos[['mmsi','port_before','port_after']].head(10),\n    headers='keys',\n    tablefmt='psql'\n))\n```\n\n\n**“Todos los registros de viajes de puerto a puerto están completos; cada buque tiene correctamente registrado su puerto de salida y de llegada, sin información faltante ni desconocida.”**\n\n\n\n\n## Revisar consumos de energía\n\n```{python}\n#| label: eda_consumos\n#| warning: false\n#| message: false\ncols_energia = ['sum_me_ene','sum_ae_ene','sum_ab_ene']\nresumen_consumos = []\n\nfor col in cols_energia:\n    n_nulos = train_val[col].isna().sum()\n    n_cero  = (train_val[col] == 0).sum()\n    n_neg   = (train_val[col] < 0).sum()\n    resumen_consumos.append([col, n_nulos, n_cero, n_neg])\n```\n\n```{python}\n#| label: e_consumos\n#| warning: false\n#| message: false\nprint(\"Valores nulos, cero y negativos en consumos:\")\nprint(tabulate(resumen_consumos, headers=[\"Columna\",\"Nulos\",\"Ceros\",\"Negativos\"], tablefmt=\"psql\"))\n```\n\nLos consumos de energía son consistentes: ninguna columna tiene valores negativos, aunque `sum_me_ene` presenta 71 valores nulos y 13,973 registros con consumo cero, `sum_ae_ene` tiene 7,513 registros con consumo cero, y `sum_ab_ene` cuenta con 15,753 registros con consumo cero.\n\n---\n```{python}\n#| label: d_consumos\n#| warning: false\n#| message: false\nprint(\"\\nResumen estadístico de consumos energéticos:\")\nprint(tabulate(train_val[cols_energia].describe().reset_index(), headers='keys', tablefmt='psql'))\n```\nLos consumos de energía varían mucho entre los registros. Muchos viajes registran cero consumo, mientras que unos pocos tienen consumos muy altos, lo que eleva el promedio. En general, la mayoría de los viajes usan poca energía, pero existen algunos casos con consumos significativamente mayores.\n\n\n\n## Duración de viajes puerto–puerto\n\n```{python}\n#| label: eda_duracion_viajes\n#| warning: false\n#| message: false\nmmsi_validos = train_val['mmsi'].value_counts()\nmmsi_validos = mmsi_validos[mmsi_validos >= 12].index\ndf_filtrado = train_val[train_val['mmsi'].isin(mmsi_validos)].copy()\ndf_filtrado['duracion_viaje_horas'] = (\n    (df_filtrado['end_leg'] - df_filtrado['start_leg']).dt.total_seconds() / 3600\n)\n```\n\nLa duración de los tramos de viaje entre puertos varía mucho: van desde unas 44 horas hasta más de 2,000 horas, con la mayoría de los tramos entre 200 y 600 horas y un promedio de unas 430 horas\n\n```{python}\n#| label: duracion_horas\n#| warning: false\n#| message: false\nprint(\"Distribución de duración de viajes (horas):\")\nprint(tabulate(df_filtrado['duracion_viaje_horas'].describe().reset_index(),\n               headers=['Métrica','Valor'],\n               tablefmt='psql'))\n```\nLa duración de los viajes varía ampliamente, desde unas 44 horas hasta más de 2,000 horas, con la mayoría de los viajes entre 200 y 600 horas y un promedio de aproximadamente 430 horas.\n\n\n\n```{python}\n#| label: eda_frecuencia_duracion_dias\n#| warning: false\n#| message: false\ndf_filtrado['duracion_viaje_dias'] = (\n    (df_filtrado['end_leg'] - df_filtrado['start_leg']).dt.total_seconds() / (3600*24)\n).astype(int)\n\ntabla_frec = df_filtrado['duracion_viaje_dias'].value_counts().reset_index()\ntabla_frec.columns = ['Duración (días)', 'Cantidad de viajes']\ntabla_frec = tabla_frec.sort_values('Cantidad de viajes', ascending=False)\n```\n\n\n\n```{python}\n#| label: duracion_dias\n#| warning: false\n#| message: false\nprint(\"Frecuencia de duración de viajes (en días) ordenada de mayor a menor cantidad de viajes:\")\nprint(tabulate(tabla_frec, headers='keys', tablefmt='psql'))\n```\n\n\nLa mayoría de los tramos de viaje entre puertos duran entre 5 y 7 días, siendo 6 días el más frecuente. Sin embargo, también hay viajes mucho más largos, algunos de hasta 86 días, lo que muestra que mientras la mayoría de los tramos son relativamente cortos, existen casos con duraciones significativamente mayores\n\n\n\n```{python}\n#| label: eda_duracion_horas\n#| warning: false\n#| message: false\ndf_filtrado = df_filtrado[\n    df_filtrado['port_before'].notna() &\n    df_filtrado['port_after'].notna() &\n    (~df_filtrado['port_before'].str.lower().isin(['', 'desconocido'])) &\n    (~df_filtrado['port_after'].str.lower().isin(['', 'desconocido']))\n].copy()\n\ndf_filtrado['duracion_horas'] = (\n    (df_filtrado['end_leg'] - df_filtrado['start_leg']).dt.total_seconds() / 3600\n)\n\ntabla_frec_horas = df_filtrado.groupby(['port_before','port_after','duracion_horas']).size().reset_index(name='cantidad_viajes')\ntabla_frec_horas = tabla_frec_horas.sort_values('cantidad_viajes', ascending=False)\n```\n\n```{python}\n#| label: on_horas\n#| warning: false\n#| message: false\ntabla_frec_horas_top10 = tabla_frec_horas.head(10)\nprint(\"Frecuencia de duración de viajes (horas) puerto->puerto ordenada de mayor a menor cantidad de viajes:\")\nprint(tabulate(tabla_frec_horas_top10, headers='keys', tablefmt='psql'))\n```\nLos datos muestran los **tramos de viaje más frecuentes entre puertos** y su duración en horas. Por ejemplo, los viajes dentro de **Chiriquí Grande** son los más repetidos, con varias duraciones registradas (alrededor de 325–570 horas). Otros tramos frecuentes incluyen rutas como **Mariel → Puerto de Hencan** o **Santa Marta → Georgetown**, con duraciones más largas, entre 800 y 1,500 horas. Esto indica que algunos trayectos son recorridos muy a menudo, mientras que otros, aunque menos frecuentes, implican viajes más largos.\n\n\n\n```{python}\n#| label: eda_duracion_dias\n#| warning: false\n#| message: false\ndf_filtrado['duracion_dias'] = ((df_filtrado['end_leg'] - df_filtrado['start_leg']).dt.total_seconds() / (3600*24)).astype(int)\ntabla_frec_dias = df_filtrado.groupby(['port_before','port_after','duracion_dias']).size().reset_index(name='cantidad_viajes')\ntabla_frec_dias = tabla_frec_dias.sort_values('cantidad_viajes', ascending=False)\n```\n\n```{python}\n#| label: puerto\n#| warning: false\n#| message: false\ntabla_frec_dias_top10 = tabla_frec_dias.head(10)\nprint(\"Top 10 frecuencia de duración de viajes (días) puerto->puerto ordenada de mayor a menor cantidad de viajes:\")\nprint(tabulate(tabla_frec_dias_top10, headers='keys', tablefmt='psql'))\n```\nLos datos muestran los **trayectos puerto a puerto más frecuentes y su duración en días**. Por ejemplo, el viaje **Manzanillo → Kingston** es el más común, con 206 registros y una duración de 10 días. Otros trayectos frecuentes incluyen **Puerto San Antonio → Freeport** (11 días) y **Andrés LNG Terminal ↔ Buenaventura** (5–6 días). Esto refleja que algunos recorridos son muy habituales, mientras que otros, aunque menos frecuentes, pueden durar hasta varias semanas.\n\n\n## Análisis de viajes específicos puerto–puerto\n\n```{python}\n#| label: eda_inspeccionar\n#| warning: false\n#| message: false\npuerto_inicio = \"MANZANILLO\"\npuerto_fin    = \"KINGSTON\"\ndias_viaje    = 10\n\nviajes_filtrados = df_filtrado[\n    (df_filtrado['port_before'] == puerto_inicio) &\n    (df_filtrado['port_after'] == puerto_fin) &\n    (((df_filtrado['end_leg'] - df_filtrado['start_leg']).dt.total_seconds() / (3600*24)).astype(int) == dias_viaje)\n]\n\nresumen_mmsi = viajes_filtrados.groupby('mmsi').size().reset_index(name='registros_viaje')\n\nprint(f\"Barcos que hicieron el viaje {puerto_inicio} -> {puerto_fin} en {dias_viaje} días:\")\nprint(tabulate(resumen_mmsi, headers='keys', tablefmt='psql'))\n```\n\nEl análisis muestra qué barcos realizaron el trayecto **Manzanillo → Kingston en exactamente 10 días**. Por ejemplo, algunos barcos completaron el viaje varias veces, con registros que van de 11 a 30 tramos por barco. Esto indica que hay rutas muy recurrentes y que ciertos buques realizan este viaje de manera repetida dentro del periodo analizado.\n\n\n\n## Cuántos buques tienen secuencias\n\n```{python}\n#| label: n_buques\n#| warning: false\n#| message: false\nregistros_por_buque = train_val.groupby(\"mmsi\").size()\nlongitudes = [1, 5, 10, 12, 15, 20]\nresumen_secuencias = []\ntotal_buques = len(registros_por_buque)\n\nfor l in longitudes:\n    n_buques = (registros_por_buque >= l).sum()\n    porcentaje = n_buques / total_buques * 100\n    resumen_secuencias.append([l, n_buques, porcentaje])\n\nresumen_df = pd.DataFrame(resumen_secuencias, columns=['Min Registros', 'N° Buques', 'Porcentaje (%)'])\nprint(resumen_df)\n```\n\nEl análisis muestra **cuántos buques tienen distintos niveles de registros en los datos**. Todos los buques (2,754) aparecen al menos una vez, pero conforme pedimos más registros por buque, el número disminuye: alrededor de la mitad tiene 5 o más registros, un cuarto tiene 10 o más, y solo un pequeño grupo (11 %) tiene 20 o más registros. Esto indica que mientras muchos buques aparecen pocas veces, algunos cuentan con series de datos más largas y consistentes.\n\n\n```{python}\n#| label: registros_por\n#| warning: false\n#| message: false\nlongitudes = [5, 6, 12, 15]\nregistros_por_buque = train_val.groupby(\"mmsi\").size()\n\nfor n in longitudes:\n    n_buques = (registros_por_buque >= n).sum()\n    print(f\"Buques con al menos {n} registros: {n_buques}\")\n```\n\n```{python}\n#| label: registros_por_buque\n#| warning: false\n#| message: false\nlongitudes = [ 3,4, 5,6,7,8, 12]\n\nfor n in longitudes:\n    buques_validos = registros_por_buque[registros_por_buque >= n].index\n    df_filtrado = train_val[train_val['mmsi'].isin(buques_validos)]\n    n_filas = len(df_filtrado)\n    n_buques = len(buques_validos)\n    print(f\"Secuencia {n}: {n_buques} buques, {n_filas} filas\")\n```\n\n\n\n\n## Creación de la variable objetivo: CO2\\_emission\n\n```{python}\n#| label: crear_target_emisiones\n#| warning: false\n#| message: false\ndef crear_target_emisiones(df, sfoc_me=0.2, sfoc_ae=0.22, sfoc_ab=0.25, factor_emision=3.17):\n    df = df.copy()\n    df['combustible_me'] = df['sum_me_ene'] * sfoc_me\n    df['combustible_ae'] = df['sum_ae_ene'] * sfoc_ae\n    df['combustible_ab'] = df['sum_ab_ene'] * sfoc_ab\n    df['fuel_consumption'] = df['combustible_me'] + df['combustible_ae'] + df['combustible_ab']\n    df['CO2_emission'] = df['fuel_consumption'] * factor_emision\n    return df\n\n```\n\n\n```{python}\n#| label: target_emisiones\n#| warning: false\n#| message: false\n\ntrain_val_filtrado = crear_target_emisiones(train_val)\ntarget = 'CO2_emission'\n```\n\n\n\n\n### **Bloque 1: Ver ceros en CO2\\_emission**\n\n```{python}\n#| label: inspeccionar_ceros\n#| warning: false\n#| message: false\n\n# Contar ceros en la variable objetivo\nceros_target = (train_val_filtrado[target] == 0).sum()\nporc_ceros = ceros_target / len(train_val_filtrado) * 100\n\nprint(f\"Registros con CO2_emission = 0: {ceros_target} ({porc_ceros:.2f}%)\")\n\n# Inspeccionar algunos registros con cero\nprint(\"\\nPrimeros registros con CO2_emission = 0:\")\nprint(train_val_filtrado[train_val_filtrado[target] == 0].head())\n```\n\n\n\n```{python}\n#| label: mmsi_con_ceros\n#| warning: false\n#| message: false\n\n# Filtrar registros con CO2_emission = 0\ndf_ceros = train_val_filtrado[train_val_filtrado[target] == 0].copy()\n\n# Cuántos MMSI distintos tienen registros con cero\nmmsi_ceros = df_ceros['mmsi'].nunique()\nprint(f\"Número de buques (mmsi) con al menos un CO2_emission = 0: {mmsi_ceros}\")\n\n# Contar registros con cero por buque\nregistros_ceros_por_mmsi = df_ceros.groupby('mmsi').size().reset_index(name='registros_cero')\nprint(\"\\nRegistros con CO2_emission = 0 por buque (top 10):\")\nprint(registros_ceros_por_mmsi.sort_values('registros_cero', ascending=False).head(10))\n\n```\n\n\n```{python}\n#| label: mmsi_con_ceros_todos\n#| warning: false\n#| message: false\n\nprint(registros_ceros_por_mmsi.sort_values('registros_cero', ascending=False))\n\n```\n\n\n\n```{python}\n#| label: resumen_mmsi_ceros\n#| warning: false\n#| message: false\n\n# Total de buques únicos\ntotal_mmsi = train_val_filtrado['mmsi'].nunique()\n\n# Buques con al menos un cero\nmmsi_con_ceros = train_val_filtrado.loc[train_val_filtrado['CO2_emission'] == 0, 'mmsi'].nunique()\n\n# Buques sin ceros\nmmsi_sin_ceros = total_mmsi - mmsi_con_ceros\n\n# Armar tabla resumen\nresumen_mmsi = pd.DataFrame({\n    \"grupo\": [\"Con ceros\", \"Sin ceros\"],\n    \"count_mmsi\": [mmsi_con_ceros, mmsi_sin_ceros],\n    \"porcentaje\": [mmsi_con_ceros/total_mmsi*100, mmsi_sin_ceros/total_mmsi*100]\n})\n\nprint(f\"Total buques únicos (mmsi): {total_mmsi}\\n\")\nprint(resumen_mmsi.to_string(index=False, formatters={\"porcentaje\": \"{:.2f}%\".format}))\n\n```\ncasi 3 de cada 10 buques pasan por situaciones de “cero emisiones” (posible parada, fondeo, error o falta de consumo registrado).\n\n### **Eliminar registros con cero**\n\n```{python}\n#| label: eliminar_ceros\n#| warning: false\n#| message: false\n\n# Eliminar filas donde CO2_emission es cero\ntrain_val_filtrado = train_val_filtrado[train_val_filtrado[target] != 0].copy()\n\nprint(f\"Filas después de eliminar ceros en CO2_emission: {len(train_val_filtrado)}\")\n```\n\n\n\n\n## Limpieza de NaN en target y verificación de secuencias mínimas\n\n```{python}\n#| label: limpieza_nan_target\n#| warning: false\n#| message: false\n# Filtrar filas donde CO2_emission es NaN\ntrain_val_target = train_val_filtrado[train_val_filtrado[target].notna()].copy()\n\nseq_len = 4  # longitud mínima de secuencia\n\n# Contar buques válidos antes y después de eliminar NaN\nregistros_por_buque_orig = train_val_filtrado.groupby('mmsi').size()\nbuques_validos_orig = (registros_por_buque_orig >= seq_len).sum()\n\nregistros_por_buque_filtrado = train_val_target.groupby('mmsi').size()\nbuques_validos_filtrado = (registros_por_buque_filtrado >= seq_len).sum()\n\nresumen = pd.DataFrame({\n    'Dataset': ['Original', 'Filtrado por target'],\n    'Buques válidos': [buques_validos_orig, buques_validos_filtrado],\n    'Filas totales': [len(train_val_filtrado), len(train_val_target)]\n})\n\nprint(tabulate(resumen, headers='keys', tablefmt='psql'))\n```\n\nAl filtrar los valores faltantes, quedarán 19,961 registros y 2,074 buques válidos. Esto significará que se habrán eliminado 71 registros individuales y 12 buques completos que ya no tendrán suficientes datos para formar secuencias mínimas.\n\n\n\n## Recorte de buques que no cumplen la longitud mínima\n\n```{python}\n#| label: recorte_buques_min\n#| warning: false\n#| message: false\n# Función para filtrar buques por cantidad mínima de registros\ndef filtrar_buques_por_min_registros(df, min_registros=4):\n    registros_por_buque = df.groupby(\"mmsi\").size()\n    buques_validos = registros_por_buque[registros_por_buque >= min_registros].index\n    return df[df['mmsi'].isin(buques_validos)].copy()\n\n\n```\n\n\n```{python}\n#| label: recorte_buques\n#| warning: false\n#| message: false\ntrain_val_filtrado = filtrar_buques_por_min_registros(train_val_target, min_registros=seq_len)\nprint(f\"Filas después del recorte por secuencia mínima: {len(train_val_filtrado)}\")\n```\n\n\n## valores nulos\n```{python}\n#| label: valores_nulos\n#| warning: false\n#| message: false\nnulos_por_col = train_val_filtrado.isnull().sum()\nnulos_filtrados = nulos_por_col[nulos_por_col > 0]\n\nresumen_nulos = pd.DataFrame({\n    'Columna': nulos_filtrados.index,\n    'Valores nulos': nulos_filtrados.values,\n    'Porcentaje nulos (%)': nulos_filtrados / len(train_val_filtrado) * 100\n}).sort_values(by='Porcentaje nulos (%)', ascending=False)\n\nprint(tabulate(resumen_nulos, headers='keys', tablefmt='grid', showindex=False))\n\n```\n\n\n```{python}\n#| label: filas_train_val_filtrado\n#| warning: false\n#| message: false\nfrom tabulate import tabulate\n\n# Crear un pequeño DataFrame resumen\nresumen_filas = pd.DataFrame({\n    'Dataset': ['train_val_filtrado'],\n    'Filas totales': [len(train_val_filtrado)]\n})\n\n# Imprimir en formato tabulate\nprint(tabulate(resumen_filas, headers='keys', tablefmt='psql', showindex=False))\n\n```\n\nDespués de aplicar los filtros sobre el conjunto de datos, se eliminarán los registros con valores nulos en la variable objetivo CO2_emission y aquellos donde esta sea igual a cero. Tras estas operaciones, el dataset train_val_filtrado contendrá 19,140 filas, correspondientes a los buques que cumplen con los criterios mínimos de secuencia. Esto permitirá que los análisis y modelos posteriores se enfoquen únicamente en datos válidos y consistentes.\n\n\n\n\n## Partición interna\n\n\n```{python}\n#| label: particion_interna\n#| warning: false\n#| message: false\nfrom sklearn.model_selection import train_test_split\n\n# Definir X (features) y y (target)\nX = train_val_filtrado.drop(columns=[target])\ny = train_val_filtrado[target]\n\n# Partición interna: entrenamiento interno y validación interna\nX_train_int, X_val_int, y_train_int, y_val_int = train_test_split(\n    X, y,\n    test_size=0.2,      # 20% para validación interna\n    random_state=42,    # para reproducibilidad\n    shuffle=True\n)\n\nprint(f\"Tamaño X_train_int: {X_train_int.shape}\")\nprint(f\"Tamaño X_val_int: {X_val_int.shape}\")\nprint(f\"Tamaño y_train_int: {y_train_int.shape}\")\nprint(f\"Tamaño y_val_int: {y_val_int.shape}\")\n\n```\n\n\n\n###  Definición de la función de imputación\n\n```{python}\n#| label: imputacion_def\n#| warning: false\n#| message: false\ndef imputar_datos(df, columnas_numericas, columnas_categoricas, flags_nulos=None, medianas=None, modas=None):\n    \"\"\"\n    Imputa valores faltantes en un DataFrame.\n    \n    Args:\n        df (pd.DataFrame): DataFrame a imputar.\n        columnas_numericas (list): Columnas numéricas a imputar con mediana.\n        columnas_categoricas (list): Columnas categóricas a imputar con moda.\n        flags_nulos (list, optional): Columnas para crear flag de valores nulos originales.\n        medianas (dict, optional): Median values para usar (train stats).\n        modas (dict, optional): Moda values para usar (train stats).\n        \n    Returns:\n        df_imputado (pd.DataFrame): DataFrame imputado.\n        medianas (dict): Median values calculadas si no se pasan.\n        modas (dict): Moda values calculadas si no se pasan.\n    \"\"\"\n    df_imputado = df.copy()\n    \n    # Crear flags de nulos\n    if flags_nulos:\n        for col in flags_nulos:\n            df_imputado[f\"{col}_is_missing\"] = df_imputado[col].isna().astype(int)\n    \n    # Si no vienen medianas y modas, calcular a partir del df actual\n    if medianas is None:\n        medianas = {col: df_imputado[col].median() for col in columnas_numericas}\n    if modas is None:\n        modas = {col: df_imputado[col].mode()[0] for col in columnas_categoricas}\n    \n    # Imputar columnas numéricas\n    for col in columnas_numericas:\n        df_imputado[col] = df_imputado[col].fillna(medianas[col])\n    \n    # Imputar columnas categóricas\n    for col in columnas_categoricas:\n        df_imputado[col] = df_imputado[col].fillna(modas[col])\n    \n    return df_imputado, medianas, modas\n```\n\n\n\n###  Aplicación en training (entrenamiento interno)\n\n```{python}\n#| label: imputacion_columnas\n#| warning: false\n#| message: false\ncolumnas_num = ['TotalBunkerCapacity', 'MainEngineRPM', 'FuelType1Capacity', 'ais_loa']\nflags = ['TotalBunkerCapacity']  # opcional si quieres marcar dónde estaban los nulos\ncolumnas_cat = []  # No hay columnas categóricas con nulos\n\n```\n\n\n```{python}\n#| label: imputacion_train\n#| warning: false\n#| message: false\nX_train_int_imputado, medianas, modas = imputar_datos(\n    X_train_int,\n    columnas_numericas=columnas_num,\n    columnas_categoricas=columnas_cat,\n    flags_nulos=flags\n)\n```\n\n\n\n###  Aplicación en validación \n\n```{python}\n#| label: imputacion_val\n#| warning: false\n#| message: false\nX_val_int_imputado, _, _ = imputar_datos(\n    X_val_int,\n    columnas_numericas=columnas_num,\n    columnas_categoricas=columnas_cat,\n    flags_nulos=flags,\n    medianas=medianas,\n    modas=modas\n)\n```\n\n\n\n\n\n\n\n##  creación de features (feature engineering)\n\n```{python}\n#| label: crear_features_def\n#| warning: false\n#| message: false\ndef crear_features(df):\n    \"\"\"\n    Crea nuevas variables/features a partir de un DataFrame imputado.\n    \n    Variables creadas:\n        - duration_min: duración entre posiciones consecutivas por buque\n        - viaje_puerto: identificador de viaje basado en cambio de puerto\n        - viaje_gap: identificador de viaje basado en gaps de tiempo\n        - viaje_leg: identificador de viaje basado en start_leg y end_leg\n        - frecuencia_puerto: número de veces que un puerto aparece como port_after por buque\n    \n    Args:\n        df (pd.DataFrame): DataFrame imputado con columnas necesarias (fechas, puertos, leg info)\n        \n    Returns:\n        pd.DataFrame: DataFrame con las nuevas features\n    \"\"\"\n    df = df.copy()\n    \n    # Ordenar por buque y tiempo\n    df = df.sort_values(by=['mmsi','first_dt_pos_utc']).reset_index(drop=True)\n    \n    # duration_min\n    df['duration_min'] = df.groupby('mmsi')['first_dt_pos_utc'].diff().dt.total_seconds() / 60\n    df['duration_min'] = df['duration_min'].fillna(0)\n    \n    # viaje_puerto\n    df['viaje_change_puerto'] = (\n        (df['port_before'] != df.groupby('mmsi')['port_before'].shift(1)) |\n        (df['port_after'] != df.groupby('mmsi')['port_after'].shift(1))\n    )\n    df['viaje_puerto'] = df.groupby('mmsi')['viaje_change_puerto'].cumsum()\n    df.drop(columns=['viaje_change_puerto'], inplace=True)\n    \n    # viaje_gap\n    umbral_gap = 60\n    df['viaje_change_gap'] = (df['duration_min'] > umbral_gap).astype(int)\n    df['viaje_gap'] = df.groupby('mmsi')['viaje_change_gap'].cumsum()\n    df.drop(columns=['viaje_change_gap'], inplace=True)\n    \n    # viaje_leg\n    df['viaje_leg'] = ((df['start_leg'] != df.groupby('mmsi')['start_leg'].shift(1)) |\n                       (df['end_leg'] != df.groupby('mmsi')['end_leg'].shift(1))).cumsum()\n    \n    # frecuencia_puerto\n    freq_puerto = df.groupby(['mmsi','port_after']).size().rename('frecuencia_puerto')\n    df = df.merge(freq_puerto, how='left', on=['mmsi','port_after'])\n    \n    return df\n```\n\n\n### Aplicación a entrenamiento interno\n\n```{python}\n#| label: crear_features_train\n#| warning: false\n#| message: false\nX_train_int_features = crear_features(X_train_int_imputado)\n```\n\n\n\n### Aplicación a validación interna\n\n```{python}\n#| label: crear_features_val\n#| warning: false\n#| message: false\nX_val_int_features = crear_features(X_val_int_imputado)\n```\n\n\n\n### vista\n```{python}\n#| label: resumen_variables\n#| warning: false\n#| message: false\nfrom tabulate import tabulate\n\n# DataFrame de ejemplo: X_train_int_imputado\ndf = X_train_int_imputado.copy()\n\n# Crear lista de variables y su tipo\nvariable_tipo = [(col, str(df[col].dtype)) for col in df.columns]\n\n# Mostrar en formato tabulate\nprint(tabulate(variable_tipo, headers=['Variable', 'Tipo'], tablefmt='grid'))\n\n```\n\n\n\n\n##  Visualización de matriz de correlación (heatmap)\n\n```{python}\n#| label: visualizar_correlacion\n#| warning: false\n#| message: false\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef mostrar_matriz_correlacion(df, figsize=(12,10), cmap='coolwarm'):\n    \"\"\"\n    Genera un heatmap de la matriz de correlación solo para columnas numéricas.\n    \n    Args:\n        df (pd.DataFrame): DataFrame a analizar.\n        figsize (tuple, optional): Tamaño de la figura. Default (12,10).\n        cmap (str, optional): Colormap de Seaborn. Default 'coolwarm'.\n        \n    Retorna:\n        corr (pd.DataFrame): Matriz de correlación (solo numéricas) para uso posterior.\n    \"\"\"\n    df_numerico = df.select_dtypes(include='number')\n    corr = df_numerico.corr()\n    \n    plt.figure(figsize=figsize)\n    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=cmap, cbar=True, square=True)\n    plt.title(\"Matriz de correlación (columnas numéricas)\")\n    plt.show()\n    \n    return corr\n\n\n```\n\n\n```{python}\n# Instancia: aplicar a X_train_int_imputado\ncorr_train = mostrar_matriz_correlacion(X_train_int_imputado)\n```\n\n\n\n\n\n\n\n\n\n\n### **Definición de la función para eliminar correlación alta**\n\n```{python}\n#| label: eliminar_correlacion\n#| warning: false\n#| message: false\nimport numpy as np\n\n# Función para elegir variables altamente correlacionadas\ndef choose_one_from_highly_correlated(correlation_matrix, threshold=0.60):\n    \"\"\"\n    Identifica columnas a eliminar según correlación alta.\n    \n    Args:\n        correlation_matrix (pd.DataFrame): matriz de correlación absoluta.\n        threshold (float): umbral de correlación para eliminar variables.\n    \n    Returns:\n        list: columnas a eliminar.\n    \"\"\"\n    columns_to_remove = set()\n    for column in correlation_matrix.columns:\n        high_corr_columns = correlation_matrix[column][correlation_matrix[column] > threshold].index.tolist()\n        if column in high_corr_columns:\n            high_corr_columns.remove(column)\n        if high_corr_columns:\n            columns_to_remove.update(high_corr_columns[1:])\n    return list(columns_to_remove)\n```\n\n\n\n### **Aplicación de la función a entrenamiento y validación**\n\n```{python}\n#| label: aplicar_eliminar_correlacion\n#| warning: false\n#| message: false\nimport numpy as np\n\n# Calcular matriz de correlación absoluta solo de columnas numéricas\ncorrelation_matrix_train = X_train_int_imputado.select_dtypes(include=[np.number]).corr().abs()\n\n# Identificar columnas a eliminar según correlación\ncolumns_to_drop = choose_one_from_highly_correlated(correlation_matrix_train, threshold=0.60)\n\n# Eliminar columnas de X_train_int y X_val_int\nX_train_int_sin_corr = X_train_int_imputado.drop(columns=columns_to_drop)\nX_val_int_sin_corr = X_val_int_imputado.drop(columns=columns_to_drop)\n\n# Resumen\nprint(f\"Columnas originales en train: {X_train_int_imputado.shape[1]}\")\nprint(f\"Columnas eliminadas por alta correlación: {len(columns_to_drop)}\")\nprint(f\"Columnas después de eliminación en train: {X_train_int_sin_corr.shape[1]}\")\n```\n\n# Aplicar la eliminación de columnas seleccionadas a train y validación\n```{python}\n#| label: eliminar_variables\n#| warning: false\n#| message: false\n# eliminar de train y validación usando la lista calculada\nX_train_int_imputado = X_train_int_imputado.drop(columns=columns_to_drop)\nX_val_int_imputado   = X_val_int_imputado.drop(columns=columns_to_drop)\n\n```\n\n## variables resultante\n\n```{python}\nfrom tabulate import tabulate\n\n# Crear lista de columnas y su tipo\nvariable_tipo = [(col, str(X_train_int_imputado[col].dtype)) for col in X_train_int_imputado.columns]\n\n# Mostrar en formato tabla\nprint(tabulate(variable_tipo, headers=['Variable', 'Tipo'], tablefmt='grid'))\n\n```\n\n\n\n\n\n\n\n## outliers\n\n\n\n## **Definir features numéricas y categóricas**\n\n```{python}\n#| label: definir_features\n#| warning: false\n#| message: false\nimport pandas as pd\n\n# Columnas numéricas que vamos a escalar\n\nnumeric_features = [\n    'GrossTonnage_x', 'sum_me_ene', 'sum_ab_ene', \n    'Speedmax', 'BreadthExtreme', 'FuelType1Capacity', \n    'MainEngineRPM', 'Powerkwservice', 'combustible_ab', \n    'TotalBunkerCapacity_is_missing'\n]\n\n\n# Columnas categóricas (objetos) excluyendo h3_sequence\ncategorical_features = X_train_int_imputado.select_dtypes(include=['object']).columns.tolist()\nif 'h3_sequence' in categorical_features:\n    categorical_features.remove('h3_sequence')\n\nprint(\"Numéricas:\", numeric_features)\nprint(\"Categóricas:\", categorical_features)\n```\n\n\n\n## **Escalado de features numéricas y codificación de categóricas**\n\n\n\n```{python}\n#| label: escalado_numerico\n#| warning: false\n#| message: false\n# Escalado numérico\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = X_train_int_imputado.copy()\nX_val_scaled = X_val_int_imputado.copy()\n\nX_train_scaled[numeric_features] = scaler.fit_transform(X_train_scaled[numeric_features])\nX_val_scaled[numeric_features] = scaler.transform(X_val_scaled[numeric_features])\n\nprint(\"Escalado numérico realizado ✅\")\n```\n\n\n\n```{python}\n#| label: conversion_categoricas\n#| warning: false\n#| message: false\n# Convertir todas las columnas categóricas a string (por si hay tipos mixtos)\nX_train_scaled[categorical_features] = X_train_scaled[categorical_features].astype(str)\nX_val_scaled[categorical_features] = X_val_scaled[categorical_features].astype(str)\n\nprint(\"Columnas categóricas convertidas a string ✅\")\n```\n\n\n\n\n\n\n```{python}\n#| label: codificacion_onehot\n#| warning: false\n#| message: false\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Codificación one-hot\nencoder = ColumnTransformer(\n    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],\n    remainder='drop'  # solo se procesan las columnas categóricas que definimos\n)\n\nX_train_enc = encoder.fit_transform(X_train_scaled)\nX_val_enc = encoder.transform(X_val_scaled)\n\nprint(\"Codificación One-Hot realizada ✅\")\n```\n\n\n\n\n\n```{python}\n#| label: conversion_dataframe\n#| warning: false\n#| message: false\n# Convertimos a DataFrame para inspección\nimport pandas as pd\n\nX_train_df = pd.DataFrame(X_train_enc.toarray() if hasattr(X_train_enc, \"toarray\") else X_train_enc)\nX_val_df = pd.DataFrame(X_val_enc.toarray() if hasattr(X_val_enc, \"toarray\") else X_val_enc)\n\nprint(\"Conversión a DataFrame realizada ✅\")\nprint(\"Dimensiones X_train_df:\", X_train_df.shape)\nprint(\"Dimensiones X_val_df:\", X_val_df.shape)\n```\n\n\n\n\n```{python}\n#| label: verificacion_targuet\n#| warning: false\n#| message: false\nprint(\"NaN en y_train_int:\", y_train_int.isna().sum())\nprint(\"NaN en y_val_int:\", y_val_int.isna().sum())\nprint(\"Valores únicos en y_train_int:\", y_train_int.unique())\n\n```\n\n\n\n## **Escalado de la variable objetivo**\n\n```{python}\n#| label: escalado_target\n#| warning: false\n#| message: false\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\nscaler_y = MinMaxScaler()\ny_train_scaled = scaler_y.fit_transform(y_train_int.values.reshape(-1,1))\ny_val_scaled = scaler_y.transform(y_val_int.values.reshape(-1,1))\n```\n\n\n## inspeccionar_target\n\n```{python}\n#| label: verificar_y\n#| warning: false\n#| message: false\nimport numpy as np\n\nprint(\"NaN en y_train_scaled:\", np.isnan(y_train_scaled).sum())\nprint(\"Inf en y_train_scaled:\", np.isinf(y_train_scaled).sum())\nprint(\"NaN en y_val_scaled:\", np.isnan(y_val_scaled).sum())\nprint(\"Inf en y_val_scaled:\", np.isinf(y_val_scaled).sum())\n\nprint(\"Valores mínimos y máximos en y_train_scaled:\", y_train_scaled.min(), y_train_scaled.max())\nprint(\"Valores mínimos y máximos en y_val_scaled:\", y_val_scaled.min(), y_val_scaled.max())\n\n```\n\n\n\n\n\n## **Crear secuencias para LSTM (con índices originales)**\n\n```{python}\n#| label: crear_secuencias\n#| warning: false\n#| message: false\nseq_len = 3\n\ndef create_sequences(X, y, df_indices, seq_len=3):\n    X_seq, y_seq, idx_seq_all = [], [], []\n    for _, group in df_indices.groupby('mmsi'):\n        n = len(group)\n        if n <= seq_len:\n            continue\n        for i in range(n - seq_len):\n            idx_seq = group.index[i:i+seq_len].to_list()\n            X_seq.append(X[idx_seq])\n            y_seq.append(y[idx_seq[-1]])\n            # Guardamos el índice real del último valor de la secuencia\n            idx_seq_all.append(group.loc[idx_seq[-1], \"index_original\"])\n    return np.array(X_seq), np.array(y_seq), np.array(idx_seq_all)\n\n# Guardar índice original\nX_train_scaled_reset = X_train_scaled.reset_index(drop=True)\nX_train_scaled_reset['index_original'] = X_train_scaled_reset.index\n\nX_val_scaled_reset = X_val_scaled.reset_index(drop=True)\nX_val_scaled_reset['index_original'] = X_val_scaled_reset.index\n\n# Crear secuencias\nX_train_seq, y_train_seq, idx_train_seq = create_sequences(\n    X_train_df.values, y_train_scaled, X_train_scaled_reset, seq_len\n)\nX_val_seq, y_val_seq, idx_val_seq = create_sequences(\n    X_val_df.values, y_val_scaled, X_val_scaled_reset, seq_len\n)\n```\n\n\n\n\n\n## **Construcción y entrenamiento del modelo LSTM**\n\n```{python}\n#| label: entrenamiento_lstm\n#| warning: false\n#| message: false\n#| output: false   # Evita que Quarto intente renderizar toda la barra de progreso\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n\nmodel = Sequential()\nmodel.add(Input(shape=(seq_len, X_train_seq.shape[2])))\nmodel.add(LSTM(64, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\nhistory = model.fit(\n    X_train_seq, y_train_seq,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_val_seq, y_val_seq),\n    verbose=1\n)\n```\n\n\n\n## **Evaluación del modelo**\n\n```{python}\n#| label: evaluacion_lstm_metricas\n#| warning: false\n#| message: false\n\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# Evaluación del modelo en validación\nloss, mae = model.evaluate(X_val_seq, y_val_seq, verbose=0)\n\n# Predicciones\ny_pred_scaled = model.predict(X_val_seq, verbose=0)\ny_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))\ny_true = scaler_y.inverse_transform(y_val_seq.reshape(-1, 1))\n\n# Métricas\nmae_original = np.mean(np.abs(y_true - y_pred))\nr2 = r2_score(y_true, y_pred)\nepsilon = 1e-8\nmape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n\n# Prints resumidos y claros\nprint(f\"R²: {r2:.4f}\")\nprint(f\"MAPE: {mape:.2f}%\")\nprint(f\"Loss (scaled): {loss:.4f}\")\nprint(f\"MAE (scaled): {mae:.4f}\")\nprint(f\"MAE original: {mae_original:.2f}\")\n```\n\n```{python}\n#| label: tu_loss\n#| warning: false\n#| message: false\nimport matplotlib.pyplot as plt\n\n# Supongamos que tu entrenamiento devuelve un objeto 'history'\n# history.history['loss'] contiene el loss por época\nloss = history.history['loss']  \nval_loss = history.history.get('val_loss')  # opcional, si tienes validación\n\nplt.figure(figsize=(8,5))\nplt.plot(loss, label='Entrenamiento')\nif val_loss:\n    plt.plot(val_loss, label='Validación')\nplt.title('Evolución del Loss')\nplt.xlabel('Época')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n```\n\n\n\n## conjunto de prueba\n\n\n\n##  Crear target CO2\\_emission en test\n\n```{python}\n#| label: crear_target_test\n#| warning: false\n#| message: false\n# Crear target en test_ext\ntest_ext = crear_target_emisiones(test_ext)\ntarget = 'CO2_emission'\n```\n\n```{python}\n#| warning: false\n#| message: false\ntest_ext_filtrado = test_ext.copy()\n```\n\n##  Eliminar filas con CO2\\_emission = 0\n\n```{python}\n#| warning: false\n#| message: false\ntest_ext_filtrado = test_ext_filtrado[test_ext_filtrado[target] != 0].copy()\n```\n\n---\n\n##  Revisar cuántas filas quedan tras eliminar ceros\n\n```{python}\n#| warning: false\n#| message: false\nprint(f\"Filas en test después de eliminar ceros en CO2_emission: {len(test_ext_filtrado)}\")\n```\n\n---\n\n##  Filtrar filas con target válido y revisar buques con secuencia mínima\n\n```{python}\n#| warning: false\n#| message: false\nseq_len = 4  # longitud mínima de secuencia\n\n# Filtrar filas con target válido\ntest_ext_target = test_ext_filtrado[test_ext_filtrado['CO2_emission'].notna()].copy()\n\n# Contar buques válidos antes y después de filtrar NaN\nregistros_por_buque_orig = test_ext_filtrado.groupby('mmsi').size()\nbuques_validos_orig = (registros_por_buque_orig >= seq_len).sum()\n\nregistros_por_buque_filtrado = test_ext_target.groupby('mmsi').size()\nbuques_validos_filtrado = (registros_por_buque_filtrado >= seq_len).sum()\n\n# Resumen\nresumen_test = pd.DataFrame({\n    'Dataset': ['Original test', 'Filtrado por target'],\n    'Buques válidos': [buques_validos_orig, buques_validos_filtrado],\n    'Filas totales': [len(test_ext_filtrado), len(test_ext_target)]\n})\n\nfrom tabulate import tabulate\nprint(tabulate(resumen_test, headers='keys', tablefmt='psql'))\n```\n\n---\n\n##  Recorte de buques por secuencia mínima\n\n```{python}\n#| warning: false\n#| message: false\ntest_ext_filtrado = filtrar_buques_por_min_registros(test_ext_target, min_registros=seq_len)\nprint(f\"Filas después del recorte por secuencia mínima en test: {len(test_ext_filtrado)}\")\n```\n\n\n```{python}\n#| warning: false\n#| message: false\n# Ver valores nulos en test_ext_filtrado\nnulos_por_col = test_ext_filtrado.isnull().sum()\nnulos_filtrados = nulos_por_col[nulos_por_col > 0]\n\nresumen_nulos = pd.DataFrame({\n    'Columna': nulos_filtrados.index,\n    'Valores nulos': nulos_filtrados.values,\n    'Porcentaje nulos (%)': nulos_filtrados / len(test_ext_filtrado) * 100\n}).sort_values(by='Porcentaje nulos (%)', ascending=False)\n\nfrom tabulate import tabulate\nprint(tabulate(resumen_nulos, headers='keys', tablefmt='grid', showindex=False))\n\n```\n\n\n## definir vairables de entrada \n\n```{python}\n#| warning: false\n#| message: false\nX_test = test_ext_filtrado.drop(columns=[target])\ny_test = test_ext_filtrado[target]\n\n```\n\n\n```{python}\n#| warning: false\n#| message: false\n# Columnas a imputar en test\ncolumnas_num = ['TotalBunkerCapacity', 'MainEngineRPM', 'FuelType1Capacity']\nflags = ['TotalBunkerCapacity']  # opcional: marcar dónde había nulos\ncolumnas_cat = []  # no hay categóricas con nulos\n\n# Aplicar imputación al test usando stats del train\nX_test_imputado, _, _ = imputar_datos(\n    X_test,\n    columnas_numericas=columnas_num,\n    columnas_categoricas=columnas_cat,\n    flags_nulos=flags,\n    medianas=medianas,  # de X_train_int_imputado\n    modas=modas\n)\n\n```\n\n## features\n```{python}\n#| warning: false\n#| message: false\nX_test_features = crear_features(X_test_imputado)\n\n```\n\n\n## ajustes de correlaciones\n\n```{python}\n#| warning: false\n#| message: false\nX_test_sin_corr = X_test_imputado.drop(columns=columns_to_drop)\n\nprint(f\"Columnas en test antes de eliminar correladas: {X_test_imputado.shape[1]}\")\nprint(f\"Columnas en test después de eliminar correladas: {X_test_sin_corr.shape[1]}\")\n\n```\n\n\n```{python}\n#| warning: false\n#| message: false\nfrom tabulate import tabulate\n\n# Resumen de columnas finales del test\nresumen_test_final = pd.DataFrame({\n    \"Columnas finales\": X_test_sin_corr.columns,\n    \"Tipo\": X_test_sin_corr.dtypes.values\n})\n\nprint(tabulate(resumen_test_final, headers=\"keys\", tablefmt=\"psql\"))\n\n```\n\n\n## features test\n\n```{python}\n#| warning: false\n#| message: false\n# Features numéricas (las que se escalan)\nnumeric_features = [\n    'GrossTonnage_x', 'sum_me_ene', 'sum_ab_ene',\n    'Speedmax', 'BreadthExtreme', 'FuelType1Capacity',\n    'MainEngineRPM', 'Powerkwservice', 'combustible_ab',\n    'TotalBunkerCapacity_is_missing'\n]\n\n# Features categóricas (objetos), excluyendo 'h3_sequence'\ncategorical_features = [\n    'port_before', 'country_before', 'port_after', 'country_after',\n    'op_phase', 'StandardVesselType_x', 'MainEngineModel', \n    'MainEngineType', 'PropulsionType', 'ShiptypeLevel5',\n    'StandardVesselType_y', 'fuel', 'meType'\n]\n\n```\n\n\n```{python}\n#| warning: false\n#| message: false\n# Escalar columnas numéricas\nX_test_scaled = X_test_sin_corr.copy()\nX_test_scaled[numeric_features] = scaler.transform(X_test_scaled[numeric_features])\n\n#  Convertir categóricas a string\nX_test_scaled[categorical_features] = X_test_scaled[categorical_features].astype(str)\n\n#  Codificación One-Hot usando encoder ya ajustado\nX_test_enc = encoder.transform(X_test_scaled)\nX_test_df = pd.DataFrame(X_test_enc.toarray() if hasattr(X_test_enc, \"toarray\") else X_test_enc)\n\n\n```\n\n\n```{python}\n#| warning: false\n#| message: false\n#  Escalar variable objetivo\ny_test_scaled = scaler_y.transform(y_test.values.reshape(-1,1))\n\n\n```\n\n\n\n```{python}\n#| warning: false\n#| message: false\n#  Crear secuencias para LSTM\nX_test_scaled_reset = X_test_scaled.reset_index(drop=True)\nX_test_scaled_reset['index_original'] = X_test_scaled_reset.index\n\nX_test_seq, y_test_seq, idx_test_seq = create_sequences(\n    X_test_df.values, y_test_scaled, X_test_scaled_reset, seq_len\n)\n\n\n```\n\n\n```{python}\n#| warning: false\n#| message: false\n#  Evaluar modelo\nloss, mae = model.evaluate(X_test_seq, y_test_seq)\nprint(\"Test Loss:\", loss, \"Test MAE:\", mae)\n```\n\n\n```{python}\n#| warning: false\n#| message: false\n# Obtener predicciones en el test\ny_test_pred = model.predict(X_test_seq)\n\n# Reconstrucción con índices originales\ndf_results_test = pd.DataFrame({\n    \"index_original\": idx_test_seq,\n    \"y_true\": y_test_seq.flatten(),\n    \"y_pred\": y_test_pred.flatten()\n})\n\n# Ver las primeras filas\ndf_results_test.head()\n\n```\n\n```{python}\n#| warning: false\n#| message: false\ndf_results_test.to_csv(\"df_results_test.csv\", index=False)\n\n```\n\n```{python}\n# Extraer mmsi para cada predicción\ndf_results_test['mmsi'] = X_test_scaled_reset.loc[df_results_test['index_original'], 'mmsi'].values\n\n# Elegir un MMSI aleatorio del conjunto de prueba\nimport numpy as np\nmmsi_random = np.random.choice(df_results_test['mmsi'].unique())\n\n# Filtrar predicciones de ese buque\ndf_buque = df_results_test[df_results_test['mmsi'] == mmsi_random]\nprint(df_buque)\n\n```\n\n```{python}\nimport matplotlib.pyplot as plt\n\n\n# Contar cuántos registros tiene cada MMSI\nconteo_buques = df_results_test['mmsi'].value_counts()\n\n# Elegir un MMSI con al menos, por ejemplo, 5 secuencias\nmmsi_ejemplo = conteo_buques[conteo_buques >= 5].index[0]\n\n# Filtrar predicciones de ese buque\ndf_buque = df_results_test[df_results_test['mmsi'] == mmsi_ejemplo]\n\n# Graficar\nplt.figure(figsize=(12,6))\nplt.plot(df_buque['y_true'].values, label='y_true', marker='o')\nplt.plot(df_buque['y_pred'].values, label='y_pred', marker='x')\nplt.title(f'Predicciones vs Valores reales para MMSI {mmsi_ejemplo}')\nplt.xlabel('Secuencia')\nplt.ylabel('CO2_emission (escalado)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n```\n\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"css":["styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Tabla de contenidos","toc-title-website":"En esta página","related-formats-title":"Otros formatos","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Fuente","other-links-title":"Otros Enlaces","code-links-title":"Enlaces de código","launch-dev-container-title":"Iniciar Dev Container","launch-binder-title":"Iniciar Binder","article-notebook-label":"Cuaderno de Artículo","notebook-preview-download":"Descargar Cuaderno","notebook-preview-download-src":"Descargar código fuente","notebook-preview-back":"Volver al Artículo","manuscript-meca-bundle":"Archivo MECA","section-title-abstract":"Resumen","section-title-appendices":"Apéndices","section-title-footnotes":"Notas","section-title-references":"Referencias","section-title-reuse":"Reutilización","section-title-copyright":"Derechos de autor","section-title-citation":"Cómo citar","appendix-attribution-cite-as":"Por favor, cita este trabajo como:","appendix-attribution-bibtex":"BibTeX","appendix-view-license":"Ver Licencia","title-block-author-single":"Autor/a","title-block-author-plural":"Autores/as","title-block-affiliation-single":"Afiliación","title-block-affiliation-plural":"Afiliaciones","title-block-published":"Fecha de publicación","title-block-modified":"Fecha de modificación","title-block-keywords":"Palabras clave","callout-tip-title":"Tip","callout-note-title":"Nota","callout-warning-title":"Advertencia","callout-important-title":"Importante","callout-caution-title":"Precaución","code-summary":"Código","code-tools-menu-caption":"Código","code-tools-show-all-code":"Mostrar todo el código","code-tools-hide-all-code":"Ocultar todo el código","code-tools-view-source":"Ver el código fuente","code-tools-source-code":"Ejecutar el código","tools-share":"Compartir","tools-download":"Descargar","code-line":"Línea","code-lines":"Líneas","copy-button-tooltip":"Copiar al portapapeles","copy-button-tooltip-success":"Copiado","repo-action-links-edit":"Editar esta página","repo-action-links-source":"Ver el código","repo-action-links-issue":"Informar de un problema","back-to-top":"Volver arriba","search-no-results-text":"Sin resultados","search-matching-documents-text":"documentos encontrados","search-copy-link-title":"Copiar el enlace en la búsqueda","search-hide-matches-text":"Ocultar resultados adicionales","search-more-match-text":"resultado adicional en este documento","search-more-matches-text":"resultados adicionales en este documento","search-clear-button-title":"Borrar","search-text-placeholder":"","search-detached-cancel-button-title":"Cancelar","search-submit-button-title":"Enviar","search-label":"Buscar","toggle-section":"Alternar sección","toggle-sidebar":"Alternar barra lateral","toggle-dark-mode":"Alternar modo oscuro","toggle-reader-mode":"Alternar modo lector","toggle-navigation":"Navegación de palanca","crossref-fig-title":"Figura","crossref-tbl-title":"Tabla","crossref-lst-title":"Listado","crossref-thm-title":"Teorema","crossref-lem-title":"Lema","crossref-cor-title":"Corolario","crossref-prp-title":"Proposición","crossref-cnj-title":"Conjetura","crossref-def-title":"Definición","crossref-exm-title":"Ejemplo","crossref-exr-title":"Ejercicio","crossref-ch-prefix":"Capítulo","crossref-apx-prefix":"Apéndice","crossref-sec-prefix":"Sección","crossref-eq-prefix":"Ecuación","crossref-lof-title":"Listado de Figuras","crossref-lot-title":"Listado de Tablas","crossref-lol-title":"Listado de Listados","environment-proof-title":"Prueba","environment-remark-title":"Observación","environment-solution-title":"Solución","listing-page-order-by":"Ordenar por","listing-page-order-by-default":"Por defecto","listing-page-order-by-date-asc":"Menos reciente","listing-page-order-by-date-desc":"Más reciente","listing-page-order-by-number-desc":"De mayor a menor","listing-page-order-by-number-asc":"De menor a mayor","listing-page-field-date":"Fecha","listing-page-field-title":"Título","listing-page-field-description":"Descripción","listing-page-field-author":"Autor/a","listing-page-field-filename":"Nombre de archivo","listing-page-field-filemodified":"Fecha de modificación","listing-page-field-subtitle":"Subtítulo","listing-page-field-readingtime":"Tiempo de lectura","listing-page-field-wordcount":"Conteo de Palabras","listing-page-field-categories":"Categorías","listing-page-minutes-compact":"{0} minutos","listing-page-category-all":"Todas","listing-page-no-matches":"No hay resultados","listing-page-words":"{0} palabras","listing-page-filter":"Filtro","draft":"Borrador"},"metadata":{"lang":"es","fig-responsive":true,"quarto-version":"1.7.32","title":"Green Maritime Corridors - Canal de Panamá","author":"Brandon Martínez","date-format":"long","abstract-title":"Resumen","abstract":"Este documento presenta un análisis de trayectorias marítimas a través del Canal de Panamá,\nincluyendo limpieza de datos, análisis exploratorio, análisis espacial H3 y modelado secuencial\ncon LSTM para la predicción de emisiones de CO₂.\n","description":"Tráfico marítimo en el Canal de Panamá, análisis técnico operacional-temporal con separación de prueba externa.","tools":"Python, pandas, matplotlib, seaborn, geopandas","link-citations":true,"document-css":false,"toc-title":"Contenidos","toc-location":"right","toc-expand":true,"lof":true,"lot":true,"math-method":"mathjax","code-layout":"full","theme":"flatly"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}